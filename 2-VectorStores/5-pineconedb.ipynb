{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a271f748",
   "metadata": {},
   "source": [
    "# Pinecone vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f37eb4",
   "metadata": {},
   "source": [
    "### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3271e224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import DB libraries\n",
    "from pinecone import Pinecone,ServerlessSpec\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Import LangChain libraries\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader, DirectoryLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e402a5b",
   "metadata": {},
   "source": [
    "### 2. Import ENV variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96af12b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "INDEX_NAME = os.getenv(\"INDEX_NAME\")\n",
    "DIMENSION = int(os.getenv(\"DIMENSION\", \"1024\"))\n",
    "CLOUD = os.getenv(\"CLOUD\", \"aws\")\n",
    "REGION = os.getenv(\"REGION\", \"us-east-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8707ec7",
   "metadata": {},
   "source": [
    "### 3. Initialize Pinecone Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88b17f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinecone index 'rag-udemy' already exists.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Pinecone\n",
    "pc = Pinecone(\n",
    "    api_key=os.getenv(\"PINECONE_API_KEY\"),\n",
    ")\n",
    "if INDEX_NAME not in pc.list_indexes().names():\n",
    "    print(f\"Creating Pinecone index: {INDEX_NAME}\")\n",
    "    pc.create_index(\n",
    "        name=INDEX_NAME,\n",
    "        dimension=DIMENSION,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=CLOUD,\n",
    "            region=REGION,\n",
    "        )\n",
    "    )\n",
    "    # Check if the index is ready\n",
    "    while not pc.describe_index(INDEX_NAME).status['ready']:\n",
    "        time.sleep(5)\n",
    "    print(f\"Pinecone index '{INDEX_NAME}' is ready.\")\n",
    "else:\n",
    "    print(f\"Pinecone index '{INDEX_NAME}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fb0d2b",
   "metadata": {},
   "source": [
    "### 4. Initialize Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5d21200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x17fdaacd0>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x17fdaabd0>, model='text-embedding-3-small', dimensions=1024, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", dimensions=1024)\n",
    "embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e196afa",
   "metadata": {},
   "source": [
    "### 5. Load and Split documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42c3d6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "news_articles = [\n",
    "    {\n",
    "        \"id\": \"news_001\",\n",
    "        \"title\": \"OpenAI Releases GPT-4 Turbo with Enhanced Capabilities\",\n",
    "        \"category\": \"news\",\n",
    "        \"source\": \"TechCrunch\",\n",
    "        \"date\": \"2024-01-15\",\n",
    "        \"content\": \"\"\"\n",
    "OpenAI has announced the release of GPT-4 Turbo, an improved version of their flagship \n",
    "language model. The new model features a larger context window of 128,000 tokens, \n",
    "significantly improved performance on complex reasoning tasks, and reduced pricing. \n",
    "GPT-4 Turbo can process the equivalent of approximately 300 pages of text in a single \n",
    "prompt, making it ideal for analyzing lengthy documents, codebases, and research papers.\n",
    "\n",
    "The model also includes improved instruction following and more consistent output formatting. \n",
    "OpenAI has reported that GPT-4 Turbo is 40% more accurate on factual questions compared to \n",
    "its predecessor and shows reduced hallucination rates. The API pricing has been reduced by \n",
    "50% for input tokens and 33% for output tokens, making it more accessible for developers \n",
    "and businesses.\n",
    "\n",
    "Key features include enhanced multimodal capabilities, better performance on mathematical \n",
    "problems, and improved code generation. The model is now available through the OpenAI API \n",
    "for enterprise customers and developers.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"news_002\",\n",
    "        \"title\": \"Google Announces Gemini: A Multimodal AI System\",\n",
    "        \"category\": \"news\",\n",
    "        \"source\": \"The Verge\",\n",
    "        \"date\": \"2024-01-20\",\n",
    "        \"content\": \"\"\"\n",
    "Google has unveiled Gemini, its most capable AI model to date, designed to be natively \n",
    "multimodal from the ground up. Unlike previous models that were adapted for multiple \n",
    "modalities, Gemini was trained on text, images, video, audio, and code simultaneously, \n",
    "allowing for more sophisticated understanding and reasoning across different types of data.\n",
    "\n",
    "Gemini comes in three versions: Gemini Ultra for highly complex tasks, Gemini Pro for \n",
    "scaling across a wide range of tasks, and Gemini Nano for on-device applications. In \n",
    "benchmark tests, Gemini Ultra exceeded human expert performance on MMLU (Massive Multitask \n",
    "Language Understanding), which tests knowledge and problem-solving abilities across 57 subjects.\n",
    "\n",
    "The model demonstrates exceptional performance in mathematics, physics, and coding tasks. \n",
    "It can understand and generate high-quality code in popular programming languages and can \n",
    "reason across text and images simultaneously. Google plans to integrate Gemini into various \n",
    "products including Search, Chrome, and Android devices, bringing advanced AI capabilities \n",
    "directly to billions of users worldwide.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"news_003\",\n",
    "        \"title\": \"AI Regulation: EU Passes Landmark Artificial Intelligence Act\",\n",
    "        \"category\": \"news\",\n",
    "        \"source\": \"Reuters\",\n",
    "        \"date\": \"2024-01-25\",\n",
    "        \"content\": \"\"\"\n",
    "The European Union has passed comprehensive AI regulation legislation, establishing the \n",
    "world's first comprehensive legal framework for artificial intelligence. The AI Act \n",
    "categorizes AI systems based on risk levels and imposes corresponding obligations on \n",
    "developers and deployers.\n",
    "\n",
    "High-risk AI systems, including those used in critical infrastructure, education, \n",
    "employment, and law enforcement, will face stringent requirements including risk \n",
    "assessments, data governance, technical documentation, and human oversight. The regulation \n",
    "explicitly bans certain AI applications deemed unacceptable, such as social scoring systems, \n",
    "real-time biometric identification in public spaces (with limited exceptions), and AI systems \n",
    "that manipulate human behavior.\n",
    "\n",
    "General-purpose AI models like GPT-4 and Claude face specific transparency requirements, \n",
    "including disclosure of training data sources and compliance with copyright law. Foundation \n",
    "model providers must conduct thorough risk assessments and report serious incidents. The \n",
    "regulation includes substantial penalties for non-compliance, with fines up to 35 million \n",
    "euros or 7% of global annual turnover. The Act will be phased in over two years, with bans \n",
    "on prohibited practices taking effect within six months.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"news_004\",\n",
    "        \"title\": \"AI Startups Raise Record $50 Billion in 2024\",\n",
    "        \"category\": \"news\",\n",
    "        \"source\": \"Bloomberg\",\n",
    "        \"date\": \"2024-02-01\",\n",
    "        \"content\": \"\"\"\n",
    "Artificial intelligence startups raised a record-breaking $50 billion in venture capital \n",
    "funding in 2024, marking a 150% increase from the previous year. The surge in investment \n",
    "reflects growing confidence in AI's transformative potential across industries and the \n",
    "success of early AI applications.\n",
    "\n",
    "Generative AI companies led the funding boom, with firms focused on text, image, video, \n",
    "and code generation attracting significant capital. Enterprise AI solutions for healthcare, \n",
    "finance, and manufacturing also saw substantial investments. Notable deals include \n",
    "Anthropic's $7 billion Series C, Cohere's $4.5 billion Series D, and numerous smaller \n",
    "rounds for specialized AI applications.\n",
    "\n",
    "Investors are particularly interested in companies building AI infrastructure, including \n",
    "vector databases, model optimization tools, and AI security solutions. The trend extends \n",
    "beyond traditional tech hubs, with AI startups in Europe, Asia, and emerging markets \n",
    "receiving unprecedented attention. However, experts caution that the market may be \n",
    "overheated, drawing comparisons to previous tech bubbles and emphasizing the importance \n",
    "of sustainable business models and real-world value creation.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"news_005\",\n",
    "        \"title\": \"Breakthrough in AI Safety: New Alignment Techniques Show Promise\",\n",
    "        \"category\": \"news\",\n",
    "        \"source\": \"MIT Technology Review\",\n",
    "        \"date\": \"2024-02-10\",\n",
    "        \"content\": \"\"\"\n",
    "Researchers at leading AI labs have announced significant progress in AI alignment \n",
    "techniques, addressing one of the field's most critical challenges. The new methods, \n",
    "called Constitutional AI and Debate-based Training, aim to ensure AI systems behave in \n",
    "accordance with human values and intentions even as they become more powerful.\n",
    "\n",
    "Constitutional AI involves training models to follow explicit principles and values, \n",
    "creating a framework for ethical decision-making. In testing, models trained with this \n",
    "approach showed dramatically reduced rates of harmful outputs while maintaining \n",
    "performance on beneficial tasks. The technique has been successfully applied to large \n",
    "language models with over 100 billion parameters.\n",
    "\n",
    "Debate-based training pits multiple AI systems against each other to identify flaws in \n",
    "reasoning and potential failure modes. This adversarial approach has proven effective at \n",
    "uncovering edge cases and vulnerabilities that traditional testing methods miss. Early \n",
    "results suggest these techniques could scale to future AI systems orders of magnitude \n",
    "more capable than current models, providing a pathway to safer artificial general intelligence.\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "tweets = [\n",
    "    {\n",
    "        \"id\": \"tweet_001\",\n",
    "        \"username\": \"@AIResearcher\",\n",
    "        \"category\": \"tweet\",\n",
    "        \"date\": \"2024-01-16\",\n",
    "        \"likes\": 15234,\n",
    "        \"retweets\": 3421,\n",
    "        \"content\": \"\"\"\n",
    "Just tested GPT-4 Turbo on our internal benchmark suite. The improvement in logical \n",
    "reasoning is remarkable - it's now solving problems that stumped GPT-4. The larger \n",
    "context window is a game-changer for document analysis. üöÄ #AI #MachineLearning\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"tweet_002\",\n",
    "        \"username\": \"@DataScientistPro\",\n",
    "        \"category\": \"tweet\",\n",
    "        \"date\": \"2024-01-18\",\n",
    "        \"likes\": 8932,\n",
    "        \"retweets\": 2156,\n",
    "        \"content\": \"\"\"\n",
    "RAG (Retrieval Augmented Generation) is the most underrated AI technique right now. \n",
    "It's solving the hallucination problem while keeping costs reasonable. Every company \n",
    "should be exploring this for their AI applications. Thread üßµüëá\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"tweet_003\",\n",
    "        \"username\": \"@MLEngineer\",\n",
    "        \"category\": \"tweet\",\n",
    "        \"date\": \"2024-01-22\",\n",
    "        \"likes\": 12445,\n",
    "        \"retweets\": 2891,\n",
    "        \"content\": \"\"\"\n",
    "Hot take: Vector databases will be as important as traditional databases in 3 years. \n",
    "The infrastructure layer for AI is forming right now, and semantic search is just the \n",
    "beginning. Pinecone, Weaviate, and Chroma are leading the charge. #VectorDB #AI\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"tweet_004\",\n",
    "        \"username\": \"@TechFounder\",\n",
    "        \"category\": \"tweet\",\n",
    "        \"date\": \"2024-01-25\",\n",
    "        \"likes\": 23567,\n",
    "        \"retweets\": 5432,\n",
    "        \"content\": \"\"\"\n",
    "We built an AI customer support system using LangChain + Pinecone + GPT-4. Results \n",
    "after 2 months:\n",
    "- 70% tickets resolved without human intervention\n",
    "- Response time down from 4hrs to 30 seconds\n",
    "- Customer satisfaction up 40%\n",
    "- Support costs down 60%\n",
    "\n",
    "AI is real. üìà\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"tweet_005\",\n",
    "        \"username\": \"@AIEthicist\",\n",
    "        \"category\": \"tweet\",\n",
    "        \"date\": \"2024-01-28\",\n",
    "        \"likes\": 9823,\n",
    "        \"retweets\": 3214,\n",
    "        \"content\": \"\"\"\n",
    "The EU AI Act is a watershed moment. While some see it as restrictive, it's actually \n",
    "providing the clarity businesses need to invest confidently in AI. Responsible AI isn't \n",
    "just ethical - it's good business. #AIRegulation #ResponsibleAI\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"tweet_006\",\n",
    "        \"username\": \"@DeepLearningDaily\",\n",
    "        \"category\": \"tweet\",\n",
    "        \"date\": \"2024-02-02\",\n",
    "        \"likes\": 18234,\n",
    "        \"retweets\": 4123,\n",
    "        \"content\": \"\"\"\n",
    "Transformer architecture turns 7 years old next month. In that time, it's:\n",
    "- Revolutionized NLP\n",
    "- Enabled GPT, BERT, and modern LLMs\n",
    "- Extended to vision (ViT) and multimodal models\n",
    "- Changed the entire AI landscape\n",
    "\n",
    "One paper to rule them all. üß† #Transformers\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"tweet_007\",\n",
    "        \"username\": \"@StartupAI\",\n",
    "        \"category\": \"tweet\",\n",
    "        \"date\": \"2024-02-05\",\n",
    "        \"likes\": 14567,\n",
    "        \"retweets\": 3789,\n",
    "        \"content\": \"\"\"\n",
    "Stop building \"ChatGPT wrappers\" and start building real AI products:\n",
    "1. Solve a specific problem deeply\n",
    "2. Fine-tune or use RAG for domain expertise\n",
    "3. Build defensible data moats\n",
    "4. Focus on user experience, not just the model\n",
    "\n",
    "The AI gold rush needs more pickaxe sellers. ‚õèÔ∏è\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"tweet_008\",\n",
    "        \"username\": \"@PromptEngineer\",\n",
    "        \"category\": \"tweet\",\n",
    "        \"date\": \"2024-02-08\",\n",
    "        \"likes\": 11234,\n",
    "        \"retweets\": 2567,\n",
    "        \"content\": \"\"\"\n",
    "Prompt engineering tips that actually work:\n",
    "- Be specific and detailed\n",
    "- Use examples (few-shot learning)\n",
    "- Break complex tasks into steps\n",
    "- Ask the model to explain its reasoning\n",
    "- Iterate based on outputs\n",
    "\n",
    "Treat it like teaching a smart intern. üí° #PromptEngineering\n",
    "        \"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc2f2bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_posts = [\n",
    "    {\n",
    "        \"id\": \"blog_001\",\n",
    "        \"title\": \"Understanding RAG: The Future of AI-Powered Applications\",\n",
    "        \"category\": \"blog\",\n",
    "        \"author\": \"Sarah Chen\",\n",
    "        \"website\": \"AI Engineering Blog\",\n",
    "        \"date\": \"2024-01-12\",\n",
    "        \"tags\": [\"RAG\", \"LLM\", \"Vector Database\", \"AI Engineering\"],\n",
    "        \"word_count\": 847,\n",
    "        \"reading_time_minutes\": 5,\n",
    "        \"sentiment\": \"positive\",\n",
    "        \"difficulty\": \"intermediate\",\n",
    "        \"content\": \"\"\"\n",
    "Retrieval-Augmented Generation (RAG) has emerged as one of the most practical and powerful \n",
    "techniques for building AI applications. In this comprehensive guide, we'll explore what \n",
    "RAG is, why it matters, and how to implement it effectively.\n",
    "\n",
    "What is RAG?\n",
    "\n",
    "RAG combines the power of large language models with external knowledge retrieval. Instead \n",
    "of relying solely on the information encoded in the model's parameters during training, \n",
    "RAG systems retrieve relevant information from a knowledge base and use it to generate \n",
    "more accurate, up-to-date, and contextually appropriate responses.\n",
    "\n",
    "The RAG Architecture\n",
    "\n",
    "A typical RAG system consists of three main components:\n",
    "\n",
    "1. Document Processing: Documents are split into chunks, converted into embeddings using \n",
    "models like OpenAI's text-embedding-3-small, and stored in a vector database such as \n",
    "Pinecone, Weaviate, or Chroma.\n",
    "\n",
    "2. Retrieval: When a user asks a question, the query is embedded and used to find the \n",
    "most relevant document chunks through semantic similarity search.\n",
    "\n",
    "3. Generation: The retrieved context is combined with the user's query and sent to a \n",
    "large language model (like GPT-4, Claude, or Llama 2) to generate a grounded response.\n",
    "\n",
    "Why RAG Matters\n",
    "\n",
    "RAG addresses several key limitations of standalone LLMs:\n",
    "- Reduces hallucinations by grounding responses in factual sources\n",
    "- Enables access to current information without retraining\n",
    "- Allows for domain-specific knowledge without fine-tuning\n",
    "- Provides transparency through source attribution\n",
    "- More cost-effective than constantly retraining models\n",
    "\n",
    "Implementation Best Practices\n",
    "\n",
    "Based on deploying dozens of RAG systems in production, here are key best practices:\n",
    "\n",
    "Chunking Strategy: Use semantic chunking rather than fixed-size splits. Aim for chunks \n",
    "between 500-1000 tokens with 10-20% overlap to maintain context.\n",
    "\n",
    "Embedding Models: Choose models based on your use case. OpenAI's embeddings offer great \n",
    "quality, while open-source options like all-MiniLM-L6-v2 provide cost savings for \n",
    "high-volume applications.\n",
    "\n",
    "Retrieval Configuration: Start with k=3-5 retrieved chunks. Use MMR (Maximum Marginal \n",
    "Relevance) for diverse results. Consider hybrid search combining semantic and keyword-based \n",
    "retrieval for better precision.\n",
    "\n",
    "Prompt Engineering: Include clear instructions for using retrieved context. Ask the model \n",
    "to cite sources and admit when information is insufficient. Structure prompts to minimize \n",
    "hallucination.\n",
    "\n",
    "Evaluation: Measure both retrieval quality (recall, precision) and generation quality \n",
    "(accuracy, relevance, coherence). Use human evaluation alongside automated metrics.\n",
    "\n",
    "Common Pitfalls to Avoid\n",
    "\n",
    "1. Poor chunking leading to incomplete context\n",
    "2. Using too many or too few retrieved chunks\n",
    "3. Not handling retrieval failures gracefully\n",
    "4. Ignoring latency optimization\n",
    "5. Insufficient testing on edge cases\n",
    "\n",
    "The Future of RAG\n",
    "\n",
    "As RAG systems mature, we're seeing exciting developments:\n",
    "- Agentic RAG with iterative retrieval\n",
    "- Multi-modal RAG combining text, images, and structured data\n",
    "- Conversational RAG maintaining context across turns\n",
    "- Self-RAG where models evaluate and refine retrievals\n",
    "\n",
    "RAG is not just a temporary solution - it represents a fundamental approach to building \n",
    "reliable, maintainable AI systems. Whether you're building a customer support chatbot, \n",
    "a research assistant, or an enterprise knowledge base, RAG should be in your toolkit.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"blog_002\",\n",
    "        \"title\": \"Vector Databases Explained: A Comprehensive Guide\",\n",
    "        \"category\": \"blog\",\n",
    "        \"author\": \"Michael Rodriguez\",\n",
    "        \"website\": \"Database Insider\",\n",
    "        \"date\": \"2024-01-18\",\n",
    "        \"tags\": [\"Vector Database\", \"Embeddings\", \"Semantic Search\", \"Infrastructure\"],\n",
    "        \"word_count\": 923,\n",
    "        \"reading_time_minutes\": 6,\n",
    "        \"sentiment\": \"neutral\",\n",
    "        \"difficulty\": \"intermediate\",\n",
    "        \"content\": \"\"\"\n",
    "Vector databases have become essential infrastructure for AI applications. This guide \n",
    "covers everything you need to know about vector databases, from basic concepts to \n",
    "production deployment.\n",
    "\n",
    "What Are Vector Databases?\n",
    "\n",
    "Vector databases are specialized systems designed to store, index, and query high-dimensional \n",
    "vector embeddings. Unlike traditional databases that store and query structured data, vector \n",
    "databases excel at semantic similarity search, finding items that are conceptually similar \n",
    "even if they don't share exact keywords.\n",
    "\n",
    "How Vector Embeddings Work\n",
    "\n",
    "Embeddings are dense numerical representations of data (text, images, audio) that capture \n",
    "semantic meaning. Similar items have similar embeddings, which can be measured using distance \n",
    "metrics like cosine similarity, Euclidean distance, or dot product. For example, the \n",
    "embeddings for \"king\" and \"monarch\" would be very close in vector space, while \"king\" and \n",
    "\"banana\" would be distant.\n",
    "\n",
    "Key Features of Vector Databases\n",
    "\n",
    "Modern vector databases provide several critical capabilities:\n",
    "\n",
    "Approximate Nearest Neighbor (ANN) Search: Efficiently find similar vectors among millions \n",
    "or billions of entries using algorithms like HNSW (Hierarchical Navigable Small World) or \n",
    "IVF (Inverted File Index).\n",
    "\n",
    "Metadata Filtering: Combine semantic search with traditional filtering, like finding similar \n",
    "documents from a specific date range or category.\n",
    "\n",
    "Scalability: Handle massive datasets with horizontal scaling and distributed architectures.\n",
    "\n",
    "Real-time Updates: Support continuous ingestion and immediate availability of new vectors.\n",
    "\n",
    "Hybrid Search: Combine vector similarity with keyword search and business logic.\n",
    "\n",
    "Popular Vector Databases\n",
    "\n",
    "Pinecone: Fully managed, serverless vector database with excellent performance and ease of \n",
    "use. Great for production applications requiring high availability.\n",
    "\n",
    "Weaviate: Open-source vector database with strong GraphQL support and multimodal capabilities. \n",
    "Offers both cloud and self-hosted options.\n",
    "\n",
    "Chroma: Lightweight, embeddable vector database perfect for development and small to \n",
    "medium-scale applications. Easy to get started with.\n",
    "\n",
    "Qdrant: High-performance open-source vector database with advanced filtering and a focus on \n",
    "efficiency. Good for self-hosted deployments.\n",
    "\n",
    "Milvus: Highly scalable open-source vector database designed for massive datasets and high \n",
    "throughput scenarios.\n",
    "\n",
    "Use Cases\n",
    "\n",
    "Vector databases power numerous AI applications:\n",
    "- Semantic search engines\n",
    "- Recommendation systems\n",
    "- RAG systems for question answering\n",
    "- Duplicate detection and deduplication\n",
    "- Anomaly detection\n",
    "- Image and video similarity search\n",
    "- Personalization engines\n",
    "\n",
    "Performance Considerations\n",
    "\n",
    "When choosing a vector database, consider:\n",
    "\n",
    "Latency: Query response time is critical for user-facing applications. Aim for sub-100ms \n",
    "latency for most use cases.\n",
    "\n",
    "Throughput: How many queries per second can the system handle? This affects cost and \n",
    "scalability.\n",
    "\n",
    "Accuracy vs Speed: ANN algorithms trade perfect accuracy for speed. Configure based on your \n",
    "tolerance for approximate results.\n",
    "\n",
    "Cost: Factor in storage costs, compute costs, and data transfer costs. Serverless options \n",
    "can be more economical for variable workloads.\n",
    "\n",
    "Best Practices for Production\n",
    "\n",
    "1. Choose appropriate embedding dimensions (384-1536 typically)\n",
    "2. Implement retry logic and error handling\n",
    "3. Monitor query latency and adjust ANN parameters\n",
    "4. Use metadata filtering to narrow search space\n",
    "5. Batch operations when possible for efficiency\n",
    "6. Implement caching for frequently accessed vectors\n",
    "7. Plan for data growth and scaling needs\n",
    "\n",
    "The vector database market is rapidly evolving, with new features and optimizations \n",
    "appearing regularly. As AI applications become more sophisticated, vector databases will \n",
    "play an increasingly central role in the infrastructure stack.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"blog_003\",\n",
    "        \"title\": \"Prompt Engineering: From Basics to Advanced Techniques\",\n",
    "        \"category\": \"blog\",\n",
    "        \"author\": \"Emily Watson\",\n",
    "        \"website\": \"AI Practitioner\",\n",
    "        \"date\": \"2024-01-24\",\n",
    "        \"tags\": [\"Prompt Engineering\", \"LLM\", \"AI\", \"Best Practices\"],\n",
    "        \"word_count\": 1056,\n",
    "        \"reading_time_minutes\": 7,\n",
    "        \"sentiment\": \"positive\",\n",
    "        \"difficulty\": \"beginner\",\n",
    "        \"content\": \"\"\"\n",
    "Prompt engineering has evolved from a curiosity to a critical skill for anyone working with \n",
    "large language models. This guide covers fundamental principles and advanced techniques for \n",
    "getting the best results from AI models.\n",
    "\n",
    "The Foundation: Clear Communication\n",
    "\n",
    "The most important principle in prompt engineering is clarity. LLMs are powerful but literal \n",
    "- they respond to exactly what you ask, not what you meant to ask. Start with these basics:\n",
    "\n",
    "Be Specific: Instead of \"Write about dogs,\" try \"Write a 500-word article about golden \n",
    "retrievers, focusing on their temperament, exercise needs, and suitability for families \n",
    "with children.\"\n",
    "\n",
    "Provide Context: Include relevant background information. \"You are an experienced software \n",
    "architect reviewing code for security vulnerabilities\" sets expectations for the model's \n",
    "perspective and expertise level.\n",
    "\n",
    "Define the Output Format: Specify exactly how you want the response structured. \"Provide \n",
    "your answer as a JSON object with keys for 'summary', 'pros', 'cons', and 'recommendation'\" \n",
    "removes ambiguity.\n",
    "\n",
    "Few-Shot Learning\n",
    "\n",
    "One of the most powerful techniques is providing examples of desired input-output pairs:\n",
    "\n",
    "Example:\n",
    "Classify the sentiment of these reviews:\n",
    "\n",
    "Review: \"This product exceeded my expectations!\"\n",
    "Sentiment: Positive\n",
    "\n",
    "Review: \"Terrible quality, broke after one use.\"\n",
    "Sentiment: Negative\n",
    "\n",
    "Review: \"The new software update is amazing!\"\n",
    "Sentiment: [model completes]\n",
    "\n",
    "Few-shot prompting dramatically improves accuracy, especially for specialized or nuanced \n",
    "tasks. Use 2-5 examples for most tasks, more for complex scenarios.\n",
    "\n",
    "Chain-of-Thought Prompting\n",
    "\n",
    "For reasoning tasks, ask the model to show its work:\n",
    "\n",
    "\"Solve this problem step by step, explaining your reasoning at each stage:\n",
    "If a train travels 120 miles in 2 hours, then stops for 30 minutes, then travels another \n",
    "90 miles in 1.5 hours, what is its average speed including the stop?\"\n",
    "\n",
    "This technique significantly improves performance on mathematical, logical, and analytical \n",
    "tasks.\n",
    "\n",
    "Role-Playing and Perspective\n",
    "\n",
    "Assigning a role or perspective can dramatically change output quality:\n",
    "\n",
    "\"You are a senior financial analyst with 15 years of experience in tech startups. Analyze \n",
    "this company's balance sheet and provide investment recommendations.\"\n",
    "\n",
    "The model will adopt appropriate vocabulary, consider relevant factors, and provide more \n",
    "sophisticated analysis.\n",
    "\n",
    "Constraints and Guardrails\n",
    "\n",
    "Add explicit constraints to prevent unwanted behaviors:\n",
    "\n",
    "- \"Do not include any information not explicitly stated in the provided context\"\n",
    "- \"If you're uncertain, say 'I don't know' rather than guessing\"\n",
    "- \"Avoid technical jargon; explain as if to a high school student\"\n",
    "- \"Limit your response to 3 bullet points\"\n",
    "\n",
    "Iterative Refinement\n",
    "\n",
    "Treat prompt engineering as an iterative process:\n",
    "\n",
    "1. Start with a basic prompt\n",
    "2. Analyze the output for issues\n",
    "3. Refine the prompt to address specific problems\n",
    "4. Test with multiple inputs\n",
    "5. Document what works\n",
    "\n",
    "Advanced Techniques\n",
    "\n",
    "Self-Consistency: Generate multiple responses and aggregate them for more reliable results \n",
    "on critical tasks.\n",
    "\n",
    "Constitutional AI: Define explicit principles and values for the model to follow, creating \n",
    "ethical and aligned outputs.\n",
    "\n",
    "Prompt Chaining: Break complex tasks into subtasks, using the output of one prompt as input \n",
    "to the next.\n",
    "\n",
    "Meta-Prompting: Have the model help optimize its own prompts through iterative feedback.\n",
    "\n",
    "Common Mistakes to Avoid\n",
    "\n",
    "1. Vague instructions leading to inconsistent outputs\n",
    "2. Overloading prompts with too many requirements\n",
    "3. Not providing sufficient context\n",
    "4. Failing to specify output format\n",
    "5. Not testing with edge cases\n",
    "6. Ignoring token limits and costs\n",
    "\n",
    "Tools and Frameworks\n",
    "\n",
    "Modern prompt engineering benefits from frameworks like:\n",
    "- LangChain for complex prompt chains and workflows\n",
    "- Guidance for structured output generation\n",
    "- OpenAI's Playground for rapid experimentation\n",
    "- Custom testing harnesses for evaluation\n",
    "\n",
    "The Future of Prompting\n",
    "\n",
    "As models become more capable, prompt engineering is evolving from an art to an engineering \n",
    "discipline. Expect to see:\n",
    "- Automated prompt optimization\n",
    "- Standard libraries of proven prompts\n",
    "- Better tools for testing and validation\n",
    "- Integration with software development practices\n",
    "\n",
    "Mastering prompt engineering multiplies your effectiveness with AI systems. Invest time in \n",
    "learning these techniques - it's one of the highest-leverage skills in modern technology.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"blog_004\",\n",
    "        \"title\": \"Building Production-Ready LLM Applications: Lessons Learned\",\n",
    "        \"category\": \"blog\",\n",
    "        \"author\": \"David Kim\",\n",
    "        \"website\": \"Engineering at Scale\",\n",
    "        \"date\": \"2024-02-03\",\n",
    "        \"tags\": [\"Production\", \"LLM\", \"DevOps\", \"Reliability\"],\n",
    "        \"word_count\": 1234,\n",
    "        \"reading_time_minutes\": 8,\n",
    "        \"sentiment\": \"neutral\",\n",
    "        \"difficulty\": \"advanced\",\n",
    "        \"content\": \"\"\"\n",
    "After deploying large language model applications serving millions of users, we've learned \n",
    "valuable lessons about what it takes to build production-ready AI systems. Here are the \n",
    "key insights from our journey.\n",
    "\n",
    "Architecture Decisions\n",
    "\n",
    "The first major decision is choosing between API-based models (OpenAI, Anthropic, Cohere) \n",
    "and self-hosted open-source models (Llama 2, Mistral, Falcon). API-based solutions offer \n",
    "faster time-to-market and zero infrastructure overhead, while self-hosted options provide \n",
    "more control and potentially lower costs at scale.\n",
    "\n",
    "We started with OpenAI's API and later added self-hosted models for specific use cases. \n",
    "This hybrid approach optimized for both quality and cost. Critical user-facing features \n",
    "use premium API models for best results, while background processing uses efficient \n",
    "open-source alternatives.\n",
    "\n",
    "Reliability and Error Handling\n",
    "\n",
    "LLMs can fail in various ways: API timeouts, rate limits, model errors, or inappropriate \n",
    "outputs. Production systems need comprehensive error handling:\n",
    "\n",
    "Implement exponential backoff with jitter for retries. Use circuit breakers to prevent \n",
    "cascading failures. Have fallback strategies when primary models are unavailable. Log all \n",
    "failures for analysis and improvement.\n",
    "\n",
    "We maintain a hot standby with a different provider. If OpenAI experiences issues, we \n",
    "automatically route traffic to Anthropic or our self-hosted models, ensuring continuity.\n",
    "\n",
    "Cost Management\n",
    "\n",
    "LLM costs can spiral quickly without proper controls:\n",
    "\n",
    "Cache responses for repeated queries - we achieved a 60% cache hit rate, dramatically \n",
    "reducing costs. Implement rate limiting per user to prevent abuse. Use cheaper models for \n",
    "simpler tasks - not everything needs GPT-4. Monitor token usage and set budget alerts.\n",
    "\n",
    "Our caching strategy alone saved $50,000 monthly. Redis stores embeddings and common \n",
    "responses, with TTLs based on content type. Breaking up requests into smaller chunks and \n",
    "reusing computed embeddings provided additional savings.\n",
    "\n",
    "Latency Optimization\n",
    "\n",
    "Users expect fast responses. Our optimizations:\n",
    "\n",
    "Stream responses instead of waiting for completion - users see output within 500ms. \n",
    "Parallel process independent subtasks. Optimize prompt lengths to reduce processing time. \n",
    "Use faster models when appropriate. Implement CDN caching for static context.\n",
    "\n",
    "We reduced median response time from 8 seconds to 1.2 seconds through these optimizations, \n",
    "dramatically improving user satisfaction.\n",
    "\n",
    "Quality Assurance\n",
    "\n",
    "Testing LLM applications is challenging due to non-deterministic outputs:\n",
    "\n",
    "Create comprehensive test suites with expected output patterns. Use LLMs to evaluate LLM \n",
    "outputs - GPT-4 can assess response quality automatically. Implement human review for \n",
    "critical applications. Track quality metrics over time. A/B test prompt variations.\n",
    "\n",
    "We built an evaluation framework using GPT-4 to score responses on accuracy, relevance, \n",
    "and helpfulness. This automated testing reduced manual QA time by 80% while improving \n",
    "consistency.\n",
    "\n",
    "Security and Safety\n",
    "\n",
    "Production LLM applications face unique security challenges:\n",
    "\n",
    "Implement input validation to prevent prompt injection attacks. Filter outputs for \n",
    "inappropriate content. Use rate limiting to prevent abuse. Sanitize user data before \n",
    "storage. Monitor for data exfiltration attempts.\n",
    "\n",
    "We experienced attempted prompt injection attacks within the first week of launch. \n",
    "Comprehensive input sanitization and output filtering caught 99.7% of malicious attempts.\n",
    "\n",
    "Observability\n",
    "\n",
    "Understanding system behavior is crucial:\n",
    "\n",
    "Log all prompts and completions (with proper data handling). Track token usage, latency, \n",
    "and costs per request. Monitor model quality metrics. Set up alerts for anomalies. Create \n",
    "dashboards for key metrics.\n",
    "\n",
    "Our monitoring caught a subtle prompt regression that reduced accuracy by 8% - something \n",
    "manual testing had missed. Continuous monitoring is non-negotiable.\n",
    "\n",
    "User Experience\n",
    "\n",
    "The best technical implementation fails without good UX:\n",
    "\n",
    "Set clear expectations about AI capabilities and limitations. Provide feedback during \n",
    "processing. Allow users to refine queries easily. Show sources and reasoning when possible. \n",
    "Enable easy reporting of issues.\n",
    "\n",
    "We added a \"regenerate\" button and thumbs up/down feedback, which provided invaluable data \n",
    "for improvement while empowering users.\n",
    "\n",
    "Continuous Improvement\n",
    "\n",
    "LLM applications require ongoing optimization:\n",
    "\n",
    "Analyze failure modes and update prompts. Fine-tune models on production data. Implement \n",
    "user feedback loops. Stay current with new models and techniques. Regularly audit for bias \n",
    "and fairness issues.\n",
    "\n",
    "We improved our system's accuracy by 23% over six months through continuous refinement \n",
    "based on production data and user feedback.\n",
    "\n",
    "Key Takeaways\n",
    "\n",
    "Building production LLM applications requires more than just calling an API. Success demands \n",
    "careful architecture, robust error handling, cost management, quality assurance, security \n",
    "measures, and continuous improvement. Treat LLM integration as you would any critical system \n",
    "component - with rigorous engineering practices and thorough testing.\n",
    "\n",
    "The AI landscape evolves rapidly, but these fundamental principles remain constant. Build \n",
    "with reliability, security, and user experience at the forefront, and your LLM applications \n",
    "will deliver lasting value.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"blog_005\",\n",
    "        \"title\": \"Fine-Tuning vs RAG: Choosing the Right Approach\",\n",
    "        \"category\": \"blog\",\n",
    "        \"author\": \"Jennifer Martinez\",\n",
    "        \"website\": \"AI Engineering Blog\",\n",
    "        \"date\": \"2024-02-10\",\n",
    "        \"tags\": [\"Fine-Tuning\", \"RAG\", \"LLM\", \"Model Training\"],\n",
    "        \"word_count\": 892,\n",
    "        \"reading_time_minutes\": 6,\n",
    "        \"sentiment\": \"neutral\",\n",
    "        \"difficulty\": \"intermediate\",\n",
    "        \"content\": \"\"\"\n",
    "One of the most common questions when building AI applications is whether to use RAG or \n",
    "fine-tune a model. The answer isn't always straightforward, and often the best solution \n",
    "involves both. Let's explore when to use each approach.\n",
    "\n",
    "Understanding the Differences\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) enhances model responses by retrieving relevant \n",
    "information from external sources at inference time. Fine-tuning modifies the model's \n",
    "weights through additional training on domain-specific data. These approaches solve \n",
    "different problems and have distinct trade-offs.\n",
    "\n",
    "When to Use RAG\n",
    "\n",
    "RAG excels in scenarios where:\n",
    "\n",
    "Dynamic Information: Your knowledge base changes frequently. Product catalogs, news, \n",
    "documentation, and policies benefit from RAG since you can update the retrieval corpus \n",
    "without retraining.\n",
    "\n",
    "Transparency Required: RAG provides source attribution, showing users where information \n",
    "comes from. This is crucial for legal, medical, or financial applications.\n",
    "\n",
    "Lower Technical Overhead: RAG requires less ML expertise than fine-tuning. You don't need \n",
    "training infrastructure or extensive datasets.\n",
    "\n",
    "Cost Constraints: Fine-tuning can be expensive, especially for large models. RAG's ongoing \n",
    "costs are more predictable and often lower.\n",
    "\n",
    "Broad Knowledge Needs: When your application needs to access diverse information that \n",
    "would require massive training data to fine-tune.\n",
    "\n",
    "When to Use Fine-Tuning\n",
    "\n",
    "Fine-tuning is better when:\n",
    "\n",
    "Consistent Style/Format: You need the model to always respond in a specific way, using \n",
    "particular terminology or following strict formatting rules.\n",
    "\n",
    "Specialized Tasks: Domain-specific tasks like medical diagnosis, legal analysis, or \n",
    "technical code generation benefit from fine-tuning on expert data.\n",
    "\n",
    "Low Latency Critical: Fine-tuned models don't need retrieval steps, reducing latency. This \n",
    "matters for real-time applications.\n",
    "\n",
    "Proprietary Knowledge: When your competitive advantage comes from specialized knowledge \n",
    "embedded in the model itself.\n",
    "\n",
    "Limited Context Windows: If your use case requires more context than fits in a prompt, \n",
    "fine-tuning can encode that knowledge in weights.\n",
    "\n",
    "The Hybrid Approach\n",
    "\n",
    "Many successful applications combine both:\n",
    "\n",
    "Fine-tune a base model on your domain to establish baseline knowledge and communication \n",
    "style. Then use RAG on top for dynamic, factual information. This gives you the benefits \n",
    "of both approaches.\n",
    "\n",
    "Example: A customer service bot might be fine-tuned on your company's communication style \n",
    "and common workflows, while using RAG to retrieve current product specs, pricing, and \n",
    "policies.\n",
    "\n",
    "Practical Considerations\n",
    "\n",
    "Data Requirements:\n",
    "- RAG: Needs organized, retrievable documents. Quality matters more than quantity.\n",
    "- Fine-tuning: Requires hundreds to thousands of high-quality training examples.\n",
    "\n",
    "Cost Structure:\n",
    "- RAG: Ongoing inference costs for embeddings and retrieval. Storage costs for vector DB.\n",
    "- Fine-tuning: Upfront training costs. Potentially lower per-request costs afterward.\n",
    "\n",
    "Maintenance:\n",
    "- RAG: Easy to update - just modify the document corpus.\n",
    "- Fine-tuning: Requires retraining for updates, which can be expensive and time-consuming.\n",
    "\n",
    "Complexity:\n",
    "- RAG: Simpler to implement initially but requires managing retrieval infrastructure.\n",
    "- Fine-tuning: Requires ML expertise and training infrastructure.\n",
    "\n",
    "Decision Framework\n",
    "\n",
    "Ask yourself these questions:\n",
    "\n",
    "1. How often does the underlying information change?\n",
    "2. Do you need source attribution?\n",
    "3. What's your ML team's expertise level?\n",
    "4. What's your budget for training vs. inference?\n",
    "5. How critical is response latency?\n",
    "6. Do you need consistent style/format?\n",
    "7. How much training data do you have?\n",
    "\n",
    "Common Mistakes\n",
    "\n",
    "Don't fine-tune when RAG would suffice: Fine-tuning is overkill for many use cases. Start \n",
    "with RAG unless you have specific reasons to fine-tune.\n",
    "\n",
    "Not considering hybrid approaches: Many assume it's either/or. Combining both often yields \n",
    "the best results.\n",
    "\n",
    "Underestimating maintenance: Fine-tuned models need regular updates as information changes. \n",
    "Plan for ongoing retraining.\n",
    "\n",
    "Ignoring evaluation: Rigorously test both approaches on your specific use case before \n",
    "committing.\n",
    "\n",
    "Getting Started\n",
    "\n",
    "Start with RAG for most applications. It's faster to implement, easier to iterate on, and \n",
    "provides transparency. Once you've validated your use case and gathered production data, \n",
    "consider whether fine-tuning would provide meaningful improvements.\n",
    "\n",
    "If you do fine-tune, start small. Fine-tune smaller models first to validate your approach \n",
    "before investing in training larger models. Use parameter-efficient techniques like LoRA \n",
    "to reduce costs.\n",
    "\n",
    "The Future\n",
    "\n",
    "The line between RAG and fine-tuning is blurring. Techniques like retrieval-enhanced \n",
    "fine-tuning and continuous learning systems combine both approaches. As models improve and \n",
    "tools mature, implementing hybrid solutions will become easier.\n",
    "\n",
    "Choose based on your specific requirements, not trends or hype. The best approach depends \n",
    "on your use case, resources, and constraints. Both RAG and fine-tuning are powerful tools \n",
    "- understanding when to use each is key to building effective AI applications.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"blog_006\",\n",
    "        \"title\": \"LangChain Deep Dive: Building Complex AI Workflows\",\n",
    "        \"category\": \"blog\",\n",
    "        \"author\": \"Alex Thompson\",\n",
    "        \"website\": \"Developer's Corner\",\n",
    "        \"date\": \"2024-02-17\",\n",
    "        \"tags\": [\"LangChain\", \"Python\", \"AI Framework\", \"Agents\"],\n",
    "        \"word_count\": 1045,\n",
    "        \"reading_time_minutes\": 7,\n",
    "        \"sentiment\": \"positive\",\n",
    "        \"difficulty\": \"intermediate\",\n",
    "        \"content\": \"\"\"\n",
    "LangChain has become the go-to framework for building sophisticated LLM applications. This \n",
    "deep dive explores its architecture, key components, and best practices for production use.\n",
    "\n",
    "What is LangChain?\n",
    "\n",
    "LangChain is an open-source framework for building applications powered by large language \n",
    "models. It provides abstractions and tools for common patterns like prompt management, \n",
    "chain composition, memory management, and agent creation. Think of it as the Rails or \n",
    "Django of LLM development - opinionated patterns that accelerate development.\n",
    "\n",
    "Core Concepts\n",
    "\n",
    "Models: LangChain supports multiple LLM providers (OpenAI, Anthropic, Hugging Face, etc.) \n",
    "through a unified interface. Switch providers without rewriting code.\n",
    "\n",
    "Prompts: Reusable prompt templates with variable substitution. Version control your prompts \n",
    "alongside code.\n",
    "\n",
    "Chains: Sequence multiple LLM calls and operations. Output from one step becomes input to \n",
    "the next.\n",
    "\n",
    "Agents: LLMs that can use tools and make decisions about which actions to take based on \n",
    "user input.\n",
    "\n",
    "Memory: Persist state across interactions for conversational applications.\n",
    "\n",
    "Building Blocks\n",
    "\n",
    "Prompt Templates:\n",
    "```python\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"product\", \"features\"],\n",
    "    template=\"Write a product description for {product} highlighting {features}\"\n",
    ")\n",
    "```\n",
    "\n",
    "Templates ensure consistency and make prompts maintainable. Use them for all but the \n",
    "simplest one-off queries.\n",
    "\n",
    "Chains:\n",
    "\n",
    "Simple chains connect operations linearly. Use LLMChain for basic prompt-to-response flows. \n",
    "SequentialChain runs multiple chains in sequence, passing outputs as inputs. More complex \n",
    "applications use custom chains with conditional logic.\n",
    "\n",
    "Example use case: A chain that summarizes a document, extracts key points, then generates \n",
    "social media posts based on those points.\n",
    "\n",
    "Agents:\n",
    "\n",
    "Agents are the most powerful but complex LangChain feature. An agent uses an LLM to decide \n",
    "which tools to use and in what order. Tools can be APIs, databases, calculators, search \n",
    "engines, or custom functions.\n",
    "\n",
    "Agent types include:\n",
    "- Zero-shot ReAct: Decides tool usage based on descriptions\n",
    "- Conversational: Maintains dialogue context\n",
    "- OpenAI Functions: Uses function calling for tool selection\n",
    "- Plan-and-Execute: Creates plans before execution\n",
    "\n",
    "Agents enable building assistants that can accomplish complex, multi-step tasks \n",
    "autonomously.\n",
    "\n",
    "Memory Management\n",
    "\n",
    "LLMs are stateless - they don't remember previous interactions. LangChain provides memory \n",
    "types to maintain context:\n",
    "\n",
    "ConversationBufferMemory: Stores entire conversation history\n",
    "ConversationSummaryMemory: Summarizes old messages to save tokens\n",
    "ConversationBufferWindowMemory: Keeps only recent N interactions\n",
    "VectorStoreMemory: Stores conversations in vector DB for semantic retrieval\n",
    "\n",
    "Choose based on your context window limits and use case requirements.\n",
    "\n",
    "RAG with LangChain\n",
    "\n",
    "LangChain simplifies RAG implementation:\n",
    "\n",
    "```python\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents, OpenAIEmbeddings())\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "```\n",
    "\n",
    "This pattern works for most RAG applications. Customize retrieval parameters and chain \n",
    "types for specific needs.\n",
    "\n",
    "Production Considerations\n",
    "\n",
    "Caching: LangChain supports caching to reduce costs and improve latency. Enable for \n",
    "repeated queries.\n",
    "\n",
    "Callbacks: Hook into chain execution for logging, monitoring, and debugging. Essential for \n",
    "production observability.\n",
    "\n",
    "Error Handling: LLMs can fail unexpectedly. Implement retry logic and fallbacks. LangChain \n",
    "provides utilities for this.\n",
    "\n",
    "Token Management: Monitor and limit token usage to control costs. Use the token counting \n",
    "utilities.\n",
    "\n",
    "Async Support: For high-throughput applications, use LangChain's async interfaces to \n",
    "improve performance.\n",
    "\n",
    "Common Pitfalls\n",
    "\n",
    "Over-chaining: Don't create complex chains when simple prompts suffice. Each chain step \n",
    "adds latency and potential failure points.\n",
    "\n",
    "Ignoring Costs: Complex chains can consume many tokens quickly. Monitor usage carefully.\n",
    "\n",
    "Not Testing Individual Components: Test each chain component independently before \n",
    "composing them.\n",
    "\n",
    "Premature Agent Usage: Agents are powerful but add complexity and unpredictability. Start \n",
    "with simpler approaches.\n",
    "\n",
    "Alternatives and Comparisons\n",
    "\n",
    "While LangChain is popular, consider alternatives:\n",
    "\n",
    "LlamaIndex: Better for RAG-focused applications with sophisticated retrieval needs.\n",
    "\n",
    "Semantic Kernel: Microsoft's framework with strong .NET support.\n",
    "\n",
    "Haystack: Focuses on NLP pipelines and document processing.\n",
    "\n",
    "Custom Solutions: For simple use cases, direct API calls might be clearer than framework \n",
    "abstractions.\n",
    "\n",
    "Choose based on your team's expertise, use case requirements, and ecosystem preferences.\n",
    "\n",
    "Advanced Patterns\n",
    "\n",
    "Self-Ask with Search: Agent asks itself clarifying questions and searches for answers.\n",
    "\n",
    "Constitutional AI: Define principles that guide agent behavior and decision-making.\n",
    "\n",
    "Multi-Agent Systems: Multiple specialized agents collaborate on complex tasks.\n",
    "\n",
    "Human-in-the-Loop: Pause execution for human review before critical actions.\n",
    "\n",
    "Getting Started\n",
    "\n",
    "Begin with simple chains before moving to complex agents. LangChain's abstractions make \n",
    "sense once you understand basic LLM patterns. If you're new to LLMs, build a few simple \n",
    "applications with direct API calls first.\n",
    "\n",
    "The documentation is comprehensive but can be overwhelming. Start with the quickstart \n",
    "guides and build progressively more complex applications.\n",
    "\n",
    "The Future\n",
    "\n",
    "LangChain evolves rapidly, with new features and improvements released frequently. The \n",
    "framework is moving toward better production-readiness with improved monitoring, evaluation \n",
    "tools, and deployment patterns.\n",
    "\n",
    "LangSmith, the companion platform, provides hosted services for prompt management, \n",
    "evaluation, and monitoring. Consider it for production applications.\n",
    "\n",
    "LangChain accelerates LLM application development but isn't magic. It's a tool that \n",
    "requires understanding both its abstractions and the underlying LLM concepts. Master both \n",
    "to build powerful, maintainable AI applications.\n",
    "        \"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2de3e6ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(blog_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b57616b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'title', 'category', 'source', 'date', 'content'])\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(news_articles[0].keys())\n",
    "print(len(news_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43361773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': 'news_001', 'title': 'OpenAI Releases GPT-4 Turbo with Enhanced Capabilities', 'category': 'news', 'date': '2024-01-15', 'source': 'TechCrunch'}, page_content='\\nOpenAI has announced the release of GPT-4 Turbo, an improved version of their flagship \\nlanguage model. The new model features a larger context window of 128,000 tokens, \\nsignificantly improved performance on complex reasoning tasks, and reduced pricing. \\nGPT-4 Turbo can process the equivalent of approximately 300 pages of text in a single \\nprompt, making it ideal for analyzing lengthy documents, codebases, and research papers.\\n\\nThe model also includes improved instruction following and more consistent output formatting. \\nOpenAI has reported that GPT-4 Turbo is 40% more accurate on factual questions compared to \\nits predecessor and shows reduced hallucination rates. The API pricing has been reduced by \\n50% for input tokens and 33% for output tokens, making it more accessible for developers \\nand businesses.\\n\\nKey features include enhanced multimodal capabilities, better performance on mathematical \\nproblems, and improved code generation. The model is now available through the OpenAI API \\nfor enterprise customers and developers.\\n        '),\n",
       " Document(metadata={'id': 'news_002', 'title': 'Google Announces Gemini: A Multimodal AI System', 'category': 'news', 'date': '2024-01-20', 'source': 'The Verge'}, page_content='\\nGoogle has unveiled Gemini, its most capable AI model to date, designed to be natively \\nmultimodal from the ground up. Unlike previous models that were adapted for multiple \\nmodalities, Gemini was trained on text, images, video, audio, and code simultaneously, \\nallowing for more sophisticated understanding and reasoning across different types of data.\\n\\nGemini comes in three versions: Gemini Ultra for highly complex tasks, Gemini Pro for \\nscaling across a wide range of tasks, and Gemini Nano for on-device applications. In \\nbenchmark tests, Gemini Ultra exceeded human expert performance on MMLU (Massive Multitask \\nLanguage Understanding), which tests knowledge and problem-solving abilities across 57 subjects.\\n\\nThe model demonstrates exceptional performance in mathematics, physics, and coding tasks. \\nIt can understand and generate high-quality code in popular programming languages and can \\nreason across text and images simultaneously. Google plans to integrate Gemini into various \\nproducts including Search, Chrome, and Android devices, bringing advanced AI capabilities \\ndirectly to billions of users worldwide.\\n        '),\n",
       " Document(metadata={'id': 'news_003', 'title': 'AI Regulation: EU Passes Landmark Artificial Intelligence Act', 'category': 'news', 'date': '2024-01-25', 'source': 'Reuters'}, page_content=\"\\nThe European Union has passed comprehensive AI regulation legislation, establishing the \\nworld's first comprehensive legal framework for artificial intelligence. The AI Act \\ncategorizes AI systems based on risk levels and imposes corresponding obligations on \\ndevelopers and deployers.\\n\\nHigh-risk AI systems, including those used in critical infrastructure, education, \\nemployment, and law enforcement, will face stringent requirements including risk \\nassessments, data governance, technical documentation, and human oversight. The regulation \\nexplicitly bans certain AI applications deemed unacceptable, such as social scoring systems, \\nreal-time biometric identification in public spaces (with limited exceptions), and AI systems \\nthat manipulate human behavior.\\n\\nGeneral-purpose AI models like GPT-4 and Claude face specific transparency requirements, \\nincluding disclosure of training data sources and compliance with copyright law. Foundation \\nmodel providers must conduct thorough risk assessments and report serious incidents. The \\nregulation includes substantial penalties for non-compliance, with fines up to 35 million \\neuros or 7% of global annual turnover. The Act will be phased in over two years, with bans \\non prohibited practices taking effect within six months.\\n        \"),\n",
       " Document(metadata={'id': 'news_004', 'title': 'AI Startups Raise Record $50 Billion in 2024', 'category': 'news', 'date': '2024-02-01', 'source': 'Bloomberg'}, page_content=\"\\nArtificial intelligence startups raised a record-breaking $50 billion in venture capital \\nfunding in 2024, marking a 150% increase from the previous year. The surge in investment \\nreflects growing confidence in AI's transformative potential across industries and the \\nsuccess of early AI applications.\\n\\nGenerative AI companies led the funding boom, with firms focused on text, image, video, \\nand code generation attracting significant capital. Enterprise AI solutions for healthcare, \\nfinance, and manufacturing also saw substantial investments. Notable deals include \\nAnthropic's $7 billion Series C, Cohere's $4.5 billion Series D, and numerous smaller \\nrounds for specialized AI applications.\\n\\nInvestors are particularly interested in companies building AI infrastructure, including \\nvector databases, model optimization tools, and AI security solutions. The trend extends \\nbeyond traditional tech hubs, with AI startups in Europe, Asia, and emerging markets \\nreceiving unprecedented attention. However, experts caution that the market may be \\noverheated, drawing comparisons to previous tech bubbles and emphasizing the importance \\nof sustainable business models and real-world value creation.\\n        \"),\n",
       " Document(metadata={'id': 'news_005', 'title': 'Breakthrough in AI Safety: New Alignment Techniques Show Promise', 'category': 'news', 'date': '2024-02-10', 'source': 'MIT Technology Review'}, page_content=\"\\nResearchers at leading AI labs have announced significant progress in AI alignment \\ntechniques, addressing one of the field's most critical challenges. The new methods, \\ncalled Constitutional AI and Debate-based Training, aim to ensure AI systems behave in \\naccordance with human values and intentions even as they become more powerful.\\n\\nConstitutional AI involves training models to follow explicit principles and values, \\ncreating a framework for ethical decision-making. In testing, models trained with this \\napproach showed dramatically reduced rates of harmful outputs while maintaining \\nperformance on beneficial tasks. The technique has been successfully applied to large \\nlanguage models with over 100 billion parameters.\\n\\nDebate-based training pits multiple AI systems against each other to identify flaws in \\nreasoning and potential failure modes. This adversarial approach has proven effective at \\nuncovering edge cases and vulnerabilities that traditional testing methods miss. Early \\nresults suggest these techniques could scale to future AI systems orders of magnitude \\nmore capable than current models, providing a pathway to safer artificial general intelligence.\\n        \")]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Convert news articles to Document objects \n",
    "news_articles_doc = []\n",
    "for article in news_articles:\n",
    "    doc = Document(\n",
    "        page_content=article[\"content\"],\n",
    "        metadata={\n",
    "            \"id\": article[\"id\"],\n",
    "            \"title\": article[\"title\"],\n",
    "            \"category\": article[\"category\"],\n",
    "            \"date\": article[\"date\"],\n",
    "            \"source\": article[\"source\"]\n",
    "        }\n",
    "    )\n",
    "    news_articles_doc.append(doc)\n",
    "news_articles_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efeb2efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'username', 'category', 'date', 'likes', 'retweets', 'content'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7364a33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': 'tweet_001', 'username': '@AIResearcher', 'date': '2024-01-16', 'retweets': 3421, 'likes': 15234}, page_content=\"\\nJust tested GPT-4 Turbo on our internal benchmark suite. The improvement in logical \\nreasoning is remarkable - it's now solving problems that stumped GPT-4. The larger \\ncontext window is a game-changer for document analysis. üöÄ #AI #MachineLearning\\n        \"),\n",
       " Document(metadata={'id': 'tweet_002', 'username': '@DataScientistPro', 'date': '2024-01-18', 'retweets': 2156, 'likes': 8932}, page_content=\"\\nRAG (Retrieval Augmented Generation) is the most underrated AI technique right now. \\nIt's solving the hallucination problem while keeping costs reasonable. Every company \\nshould be exploring this for their AI applications. Thread üßµüëá\\n        \"),\n",
       " Document(metadata={'id': 'tweet_003', 'username': '@MLEngineer', 'date': '2024-01-22', 'retweets': 2891, 'likes': 12445}, page_content='\\nHot take: Vector databases will be as important as traditional databases in 3 years. \\nThe infrastructure layer for AI is forming right now, and semantic search is just the \\nbeginning. Pinecone, Weaviate, and Chroma are leading the charge. #VectorDB #AI\\n        '),\n",
       " Document(metadata={'id': 'tweet_004', 'username': '@TechFounder', 'date': '2024-01-25', 'retweets': 5432, 'likes': 23567}, page_content='\\nWe built an AI customer support system using LangChain + Pinecone + GPT-4. Results \\nafter 2 months:\\n- 70% tickets resolved without human intervention\\n- Response time down from 4hrs to 30 seconds\\n- Customer satisfaction up 40%\\n- Support costs down 60%\\n\\nAI is real. üìà\\n        '),\n",
       " Document(metadata={'id': 'tweet_005', 'username': '@AIEthicist', 'date': '2024-01-28', 'retweets': 3214, 'likes': 9823}, page_content=\"\\nThe EU AI Act is a watershed moment. While some see it as restrictive, it's actually \\nproviding the clarity businesses need to invest confidently in AI. Responsible AI isn't \\njust ethical - it's good business. #AIRegulation #ResponsibleAI\\n        \"),\n",
       " Document(metadata={'id': 'tweet_006', 'username': '@DeepLearningDaily', 'date': '2024-02-02', 'retweets': 4123, 'likes': 18234}, page_content=\"\\nTransformer architecture turns 7 years old next month. In that time, it's:\\n- Revolutionized NLP\\n- Enabled GPT, BERT, and modern LLMs\\n- Extended to vision (ViT) and multimodal models\\n- Changed the entire AI landscape\\n\\nOne paper to rule them all. üß† #Transformers\\n        \"),\n",
       " Document(metadata={'id': 'tweet_007', 'username': '@StartupAI', 'date': '2024-02-05', 'retweets': 3789, 'likes': 14567}, page_content='\\nStop building \"ChatGPT wrappers\" and start building real AI products:\\n1. Solve a specific problem deeply\\n2. Fine-tune or use RAG for domain expertise\\n3. Build defensible data moats\\n4. Focus on user experience, not just the model\\n\\nThe AI gold rush needs more pickaxe sellers. ‚õèÔ∏è\\n        '),\n",
       " Document(metadata={'id': 'tweet_008', 'username': '@PromptEngineer', 'date': '2024-02-08', 'retweets': 2567, 'likes': 11234}, page_content='\\nPrompt engineering tips that actually work:\\n- Be specific and detailed\\n- Use examples (few-shot learning)\\n- Break complex tasks into steps\\n- Ask the model to explain its reasoning\\n- Iterate based on outputs\\n\\nTreat it like teaching a smart intern. üí° #PromptEngineering\\n        ')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert tweets to Document objects\n",
    "tweets_doc = []\n",
    "for tweet in tweets:\n",
    "    doc = Document(\n",
    "        page_content=tweet[\"content\"],\n",
    "        metadata={\n",
    "            \"id\": tweet[\"id\"],\n",
    "            \"username\": tweet[\"username\"],\n",
    "            \"date\": tweet[\"date\"],\n",
    "            \"retweets\": tweet[\"retweets\"],\n",
    "            \"likes\": tweet[\"likes\"]\n",
    "        }\n",
    "    )\n",
    "    tweets_doc.append(doc)\n",
    "tweets_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "074ad52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'title', 'category', 'author', 'website', 'date', 'tags', 'word_count', 'reading_time_minutes', 'sentiment', 'difficulty', 'content'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_posts[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a573cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': 'blog_001', 'title': 'Understanding RAG: The Future of AI-Powered Applications', 'category': 'blog', 'author': 'Sarah Chen', 'website': 'AI Engineering Blog', 'date': '2024-01-12', 'tags': ['RAG', 'LLM', 'Vector Database', 'AI Engineering'], 'word_count': 847, 'reading_time_minutes': 5, 'sentiment': 'positive', 'difficulty': 'intermediate'}, page_content=\"\\nRetrieval-Augmented Generation (RAG) has emerged as one of the most practical and powerful \\ntechniques for building AI applications. In this comprehensive guide, we'll explore what \\nRAG is, why it matters, and how to implement it effectively.\\n\\nWhat is RAG?\\n\\nRAG combines the power of large language models with external knowledge retrieval. Instead \\nof relying solely on the information encoded in the model's parameters during training, \\nRAG systems retrieve relevant information from a knowledge base and use it to generate \\nmore accurate, up-to-date, and contextually appropriate responses.\\n\\nThe RAG Architecture\\n\\nA typical RAG system consists of three main components:\\n\\n1. Document Processing: Documents are split into chunks, converted into embeddings using \\nmodels like OpenAI's text-embedding-3-small, and stored in a vector database such as \\nPinecone, Weaviate, or Chroma.\\n\\n2. Retrieval: When a user asks a question, the query is embedded and used to find the \\nmost relevant document chunks through semantic similarity search.\\n\\n3. Generation: The retrieved context is combined with the user's query and sent to a \\nlarge language model (like GPT-4, Claude, or Llama 2) to generate a grounded response.\\n\\nWhy RAG Matters\\n\\nRAG addresses several key limitations of standalone LLMs:\\n- Reduces hallucinations by grounding responses in factual sources\\n- Enables access to current information without retraining\\n- Allows for domain-specific knowledge without fine-tuning\\n- Provides transparency through source attribution\\n- More cost-effective than constantly retraining models\\n\\nImplementation Best Practices\\n\\nBased on deploying dozens of RAG systems in production, here are key best practices:\\n\\nChunking Strategy: Use semantic chunking rather than fixed-size splits. Aim for chunks \\nbetween 500-1000 tokens with 10-20% overlap to maintain context.\\n\\nEmbedding Models: Choose models based on your use case. OpenAI's embeddings offer great \\nquality, while open-source options like all-MiniLM-L6-v2 provide cost savings for \\nhigh-volume applications.\\n\\nRetrieval Configuration: Start with k=3-5 retrieved chunks. Use MMR (Maximum Marginal \\nRelevance) for diverse results. Consider hybrid search combining semantic and keyword-based \\nretrieval for better precision.\\n\\nPrompt Engineering: Include clear instructions for using retrieved context. Ask the model \\nto cite sources and admit when information is insufficient. Structure prompts to minimize \\nhallucination.\\n\\nEvaluation: Measure both retrieval quality (recall, precision) and generation quality \\n(accuracy, relevance, coherence). Use human evaluation alongside automated metrics.\\n\\nCommon Pitfalls to Avoid\\n\\n1. Poor chunking leading to incomplete context\\n2. Using too many or too few retrieved chunks\\n3. Not handling retrieval failures gracefully\\n4. Ignoring latency optimization\\n5. Insufficient testing on edge cases\\n\\nThe Future of RAG\\n\\nAs RAG systems mature, we're seeing exciting developments:\\n- Agentic RAG with iterative retrieval\\n- Multi-modal RAG combining text, images, and structured data\\n- Conversational RAG maintaining context across turns\\n- Self-RAG where models evaluate and refine retrievals\\n\\nRAG is not just a temporary solution - it represents a fundamental approach to building \\nreliable, maintainable AI systems. Whether you're building a customer support chatbot, \\na research assistant, or an enterprise knowledge base, RAG should be in your toolkit.\\n        \"),\n",
       " Document(metadata={'id': 'blog_002', 'title': 'Vector Databases Explained: A Comprehensive Guide', 'category': 'blog', 'author': 'Michael Rodriguez', 'website': 'Database Insider', 'date': '2024-01-18', 'tags': ['Vector Database', 'Embeddings', 'Semantic Search', 'Infrastructure'], 'word_count': 923, 'reading_time_minutes': 6, 'sentiment': 'neutral', 'difficulty': 'intermediate'}, page_content='\\nVector databases have become essential infrastructure for AI applications. This guide \\ncovers everything you need to know about vector databases, from basic concepts to \\nproduction deployment.\\n\\nWhat Are Vector Databases?\\n\\nVector databases are specialized systems designed to store, index, and query high-dimensional \\nvector embeddings. Unlike traditional databases that store and query structured data, vector \\ndatabases excel at semantic similarity search, finding items that are conceptually similar \\neven if they don\\'t share exact keywords.\\n\\nHow Vector Embeddings Work\\n\\nEmbeddings are dense numerical representations of data (text, images, audio) that capture \\nsemantic meaning. Similar items have similar embeddings, which can be measured using distance \\nmetrics like cosine similarity, Euclidean distance, or dot product. For example, the \\nembeddings for \"king\" and \"monarch\" would be very close in vector space, while \"king\" and \\n\"banana\" would be distant.\\n\\nKey Features of Vector Databases\\n\\nModern vector databases provide several critical capabilities:\\n\\nApproximate Nearest Neighbor (ANN) Search: Efficiently find similar vectors among millions \\nor billions of entries using algorithms like HNSW (Hierarchical Navigable Small World) or \\nIVF (Inverted File Index).\\n\\nMetadata Filtering: Combine semantic search with traditional filtering, like finding similar \\ndocuments from a specific date range or category.\\n\\nScalability: Handle massive datasets with horizontal scaling and distributed architectures.\\n\\nReal-time Updates: Support continuous ingestion and immediate availability of new vectors.\\n\\nHybrid Search: Combine vector similarity with keyword search and business logic.\\n\\nPopular Vector Databases\\n\\nPinecone: Fully managed, serverless vector database with excellent performance and ease of \\nuse. Great for production applications requiring high availability.\\n\\nWeaviate: Open-source vector database with strong GraphQL support and multimodal capabilities. \\nOffers both cloud and self-hosted options.\\n\\nChroma: Lightweight, embeddable vector database perfect for development and small to \\nmedium-scale applications. Easy to get started with.\\n\\nQdrant: High-performance open-source vector database with advanced filtering and a focus on \\nefficiency. Good for self-hosted deployments.\\n\\nMilvus: Highly scalable open-source vector database designed for massive datasets and high \\nthroughput scenarios.\\n\\nUse Cases\\n\\nVector databases power numerous AI applications:\\n- Semantic search engines\\n- Recommendation systems\\n- RAG systems for question answering\\n- Duplicate detection and deduplication\\n- Anomaly detection\\n- Image and video similarity search\\n- Personalization engines\\n\\nPerformance Considerations\\n\\nWhen choosing a vector database, consider:\\n\\nLatency: Query response time is critical for user-facing applications. Aim for sub-100ms \\nlatency for most use cases.\\n\\nThroughput: How many queries per second can the system handle? This affects cost and \\nscalability.\\n\\nAccuracy vs Speed: ANN algorithms trade perfect accuracy for speed. Configure based on your \\ntolerance for approximate results.\\n\\nCost: Factor in storage costs, compute costs, and data transfer costs. Serverless options \\ncan be more economical for variable workloads.\\n\\nBest Practices for Production\\n\\n1. Choose appropriate embedding dimensions (384-1536 typically)\\n2. Implement retry logic and error handling\\n3. Monitor query latency and adjust ANN parameters\\n4. Use metadata filtering to narrow search space\\n5. Batch operations when possible for efficiency\\n6. Implement caching for frequently accessed vectors\\n7. Plan for data growth and scaling needs\\n\\nThe vector database market is rapidly evolving, with new features and optimizations \\nappearing regularly. As AI applications become more sophisticated, vector databases will \\nplay an increasingly central role in the infrastructure stack.\\n        '),\n",
       " Document(metadata={'id': 'blog_003', 'title': 'Prompt Engineering: From Basics to Advanced Techniques', 'category': 'blog', 'author': 'Emily Watson', 'website': 'AI Practitioner', 'date': '2024-01-24', 'tags': ['Prompt Engineering', 'LLM', 'AI', 'Best Practices'], 'word_count': 1056, 'reading_time_minutes': 7, 'sentiment': 'positive', 'difficulty': 'beginner'}, page_content='\\nPrompt engineering has evolved from a curiosity to a critical skill for anyone working with \\nlarge language models. This guide covers fundamental principles and advanced techniques for \\ngetting the best results from AI models.\\n\\nThe Foundation: Clear Communication\\n\\nThe most important principle in prompt engineering is clarity. LLMs are powerful but literal \\n- they respond to exactly what you ask, not what you meant to ask. Start with these basics:\\n\\nBe Specific: Instead of \"Write about dogs,\" try \"Write a 500-word article about golden \\nretrievers, focusing on their temperament, exercise needs, and suitability for families \\nwith children.\"\\n\\nProvide Context: Include relevant background information. \"You are an experienced software \\narchitect reviewing code for security vulnerabilities\" sets expectations for the model\\'s \\nperspective and expertise level.\\n\\nDefine the Output Format: Specify exactly how you want the response structured. \"Provide \\nyour answer as a JSON object with keys for \\'summary\\', \\'pros\\', \\'cons\\', and \\'recommendation\\'\" \\nremoves ambiguity.\\n\\nFew-Shot Learning\\n\\nOne of the most powerful techniques is providing examples of desired input-output pairs:\\n\\nExample:\\nClassify the sentiment of these reviews:\\n\\nReview: \"This product exceeded my expectations!\"\\nSentiment: Positive\\n\\nReview: \"Terrible quality, broke after one use.\"\\nSentiment: Negative\\n\\nReview: \"The new software update is amazing!\"\\nSentiment: [model completes]\\n\\nFew-shot prompting dramatically improves accuracy, especially for specialized or nuanced \\ntasks. Use 2-5 examples for most tasks, more for complex scenarios.\\n\\nChain-of-Thought Prompting\\n\\nFor reasoning tasks, ask the model to show its work:\\n\\n\"Solve this problem step by step, explaining your reasoning at each stage:\\nIf a train travels 120 miles in 2 hours, then stops for 30 minutes, then travels another \\n90 miles in 1.5 hours, what is its average speed including the stop?\"\\n\\nThis technique significantly improves performance on mathematical, logical, and analytical \\ntasks.\\n\\nRole-Playing and Perspective\\n\\nAssigning a role or perspective can dramatically change output quality:\\n\\n\"You are a senior financial analyst with 15 years of experience in tech startups. Analyze \\nthis company\\'s balance sheet and provide investment recommendations.\"\\n\\nThe model will adopt appropriate vocabulary, consider relevant factors, and provide more \\nsophisticated analysis.\\n\\nConstraints and Guardrails\\n\\nAdd explicit constraints to prevent unwanted behaviors:\\n\\n- \"Do not include any information not explicitly stated in the provided context\"\\n- \"If you\\'re uncertain, say \\'I don\\'t know\\' rather than guessing\"\\n- \"Avoid technical jargon; explain as if to a high school student\"\\n- \"Limit your response to 3 bullet points\"\\n\\nIterative Refinement\\n\\nTreat prompt engineering as an iterative process:\\n\\n1. Start with a basic prompt\\n2. Analyze the output for issues\\n3. Refine the prompt to address specific problems\\n4. Test with multiple inputs\\n5. Document what works\\n\\nAdvanced Techniques\\n\\nSelf-Consistency: Generate multiple responses and aggregate them for more reliable results \\non critical tasks.\\n\\nConstitutional AI: Define explicit principles and values for the model to follow, creating \\nethical and aligned outputs.\\n\\nPrompt Chaining: Break complex tasks into subtasks, using the output of one prompt as input \\nto the next.\\n\\nMeta-Prompting: Have the model help optimize its own prompts through iterative feedback.\\n\\nCommon Mistakes to Avoid\\n\\n1. Vague instructions leading to inconsistent outputs\\n2. Overloading prompts with too many requirements\\n3. Not providing sufficient context\\n4. Failing to specify output format\\n5. Not testing with edge cases\\n6. Ignoring token limits and costs\\n\\nTools and Frameworks\\n\\nModern prompt engineering benefits from frameworks like:\\n- LangChain for complex prompt chains and workflows\\n- Guidance for structured output generation\\n- OpenAI\\'s Playground for rapid experimentation\\n- Custom testing harnesses for evaluation\\n\\nThe Future of Prompting\\n\\nAs models become more capable, prompt engineering is evolving from an art to an engineering \\ndiscipline. Expect to see:\\n- Automated prompt optimization\\n- Standard libraries of proven prompts\\n- Better tools for testing and validation\\n- Integration with software development practices\\n\\nMastering prompt engineering multiplies your effectiveness with AI systems. Invest time in \\nlearning these techniques - it\\'s one of the highest-leverage skills in modern technology.\\n        '),\n",
       " Document(metadata={'id': 'blog_004', 'title': 'Building Production-Ready LLM Applications: Lessons Learned', 'category': 'blog', 'author': 'David Kim', 'website': 'Engineering at Scale', 'date': '2024-02-03', 'tags': ['Production', 'LLM', 'DevOps', 'Reliability'], 'word_count': 1234, 'reading_time_minutes': 8, 'sentiment': 'neutral', 'difficulty': 'advanced'}, page_content='\\nAfter deploying large language model applications serving millions of users, we\\'ve learned \\nvaluable lessons about what it takes to build production-ready AI systems. Here are the \\nkey insights from our journey.\\n\\nArchitecture Decisions\\n\\nThe first major decision is choosing between API-based models (OpenAI, Anthropic, Cohere) \\nand self-hosted open-source models (Llama 2, Mistral, Falcon). API-based solutions offer \\nfaster time-to-market and zero infrastructure overhead, while self-hosted options provide \\nmore control and potentially lower costs at scale.\\n\\nWe started with OpenAI\\'s API and later added self-hosted models for specific use cases. \\nThis hybrid approach optimized for both quality and cost. Critical user-facing features \\nuse premium API models for best results, while background processing uses efficient \\nopen-source alternatives.\\n\\nReliability and Error Handling\\n\\nLLMs can fail in various ways: API timeouts, rate limits, model errors, or inappropriate \\noutputs. Production systems need comprehensive error handling:\\n\\nImplement exponential backoff with jitter for retries. Use circuit breakers to prevent \\ncascading failures. Have fallback strategies when primary models are unavailable. Log all \\nfailures for analysis and improvement.\\n\\nWe maintain a hot standby with a different provider. If OpenAI experiences issues, we \\nautomatically route traffic to Anthropic or our self-hosted models, ensuring continuity.\\n\\nCost Management\\n\\nLLM costs can spiral quickly without proper controls:\\n\\nCache responses for repeated queries - we achieved a 60% cache hit rate, dramatically \\nreducing costs. Implement rate limiting per user to prevent abuse. Use cheaper models for \\nsimpler tasks - not everything needs GPT-4. Monitor token usage and set budget alerts.\\n\\nOur caching strategy alone saved $50,000 monthly. Redis stores embeddings and common \\nresponses, with TTLs based on content type. Breaking up requests into smaller chunks and \\nreusing computed embeddings provided additional savings.\\n\\nLatency Optimization\\n\\nUsers expect fast responses. Our optimizations:\\n\\nStream responses instead of waiting for completion - users see output within 500ms. \\nParallel process independent subtasks. Optimize prompt lengths to reduce processing time. \\nUse faster models when appropriate. Implement CDN caching for static context.\\n\\nWe reduced median response time from 8 seconds to 1.2 seconds through these optimizations, \\ndramatically improving user satisfaction.\\n\\nQuality Assurance\\n\\nTesting LLM applications is challenging due to non-deterministic outputs:\\n\\nCreate comprehensive test suites with expected output patterns. Use LLMs to evaluate LLM \\noutputs - GPT-4 can assess response quality automatically. Implement human review for \\ncritical applications. Track quality metrics over time. A/B test prompt variations.\\n\\nWe built an evaluation framework using GPT-4 to score responses on accuracy, relevance, \\nand helpfulness. This automated testing reduced manual QA time by 80% while improving \\nconsistency.\\n\\nSecurity and Safety\\n\\nProduction LLM applications face unique security challenges:\\n\\nImplement input validation to prevent prompt injection attacks. Filter outputs for \\ninappropriate content. Use rate limiting to prevent abuse. Sanitize user data before \\nstorage. Monitor for data exfiltration attempts.\\n\\nWe experienced attempted prompt injection attacks within the first week of launch. \\nComprehensive input sanitization and output filtering caught 99.7% of malicious attempts.\\n\\nObservability\\n\\nUnderstanding system behavior is crucial:\\n\\nLog all prompts and completions (with proper data handling). Track token usage, latency, \\nand costs per request. Monitor model quality metrics. Set up alerts for anomalies. Create \\ndashboards for key metrics.\\n\\nOur monitoring caught a subtle prompt regression that reduced accuracy by 8% - something \\nmanual testing had missed. Continuous monitoring is non-negotiable.\\n\\nUser Experience\\n\\nThe best technical implementation fails without good UX:\\n\\nSet clear expectations about AI capabilities and limitations. Provide feedback during \\nprocessing. Allow users to refine queries easily. Show sources and reasoning when possible. \\nEnable easy reporting of issues.\\n\\nWe added a \"regenerate\" button and thumbs up/down feedback, which provided invaluable data \\nfor improvement while empowering users.\\n\\nContinuous Improvement\\n\\nLLM applications require ongoing optimization:\\n\\nAnalyze failure modes and update prompts. Fine-tune models on production data. Implement \\nuser feedback loops. Stay current with new models and techniques. Regularly audit for bias \\nand fairness issues.\\n\\nWe improved our system\\'s accuracy by 23% over six months through continuous refinement \\nbased on production data and user feedback.\\n\\nKey Takeaways\\n\\nBuilding production LLM applications requires more than just calling an API. Success demands \\ncareful architecture, robust error handling, cost management, quality assurance, security \\nmeasures, and continuous improvement. Treat LLM integration as you would any critical system \\ncomponent - with rigorous engineering practices and thorough testing.\\n\\nThe AI landscape evolves rapidly, but these fundamental principles remain constant. Build \\nwith reliability, security, and user experience at the forefront, and your LLM applications \\nwill deliver lasting value.\\n        '),\n",
       " Document(metadata={'id': 'blog_005', 'title': 'Fine-Tuning vs RAG: Choosing the Right Approach', 'category': 'blog', 'author': 'Jennifer Martinez', 'website': 'AI Engineering Blog', 'date': '2024-02-10', 'tags': ['Fine-Tuning', 'RAG', 'LLM', 'Model Training'], 'word_count': 892, 'reading_time_minutes': 6, 'sentiment': 'neutral', 'difficulty': 'intermediate'}, page_content=\"\\nOne of the most common questions when building AI applications is whether to use RAG or \\nfine-tune a model. The answer isn't always straightforward, and often the best solution \\ninvolves both. Let's explore when to use each approach.\\n\\nUnderstanding the Differences\\n\\nRAG (Retrieval-Augmented Generation) enhances model responses by retrieving relevant \\ninformation from external sources at inference time. Fine-tuning modifies the model's \\nweights through additional training on domain-specific data. These approaches solve \\ndifferent problems and have distinct trade-offs.\\n\\nWhen to Use RAG\\n\\nRAG excels in scenarios where:\\n\\nDynamic Information: Your knowledge base changes frequently. Product catalogs, news, \\ndocumentation, and policies benefit from RAG since you can update the retrieval corpus \\nwithout retraining.\\n\\nTransparency Required: RAG provides source attribution, showing users where information \\ncomes from. This is crucial for legal, medical, or financial applications.\\n\\nLower Technical Overhead: RAG requires less ML expertise than fine-tuning. You don't need \\ntraining infrastructure or extensive datasets.\\n\\nCost Constraints: Fine-tuning can be expensive, especially for large models. RAG's ongoing \\ncosts are more predictable and often lower.\\n\\nBroad Knowledge Needs: When your application needs to access diverse information that \\nwould require massive training data to fine-tune.\\n\\nWhen to Use Fine-Tuning\\n\\nFine-tuning is better when:\\n\\nConsistent Style/Format: You need the model to always respond in a specific way, using \\nparticular terminology or following strict formatting rules.\\n\\nSpecialized Tasks: Domain-specific tasks like medical diagnosis, legal analysis, or \\ntechnical code generation benefit from fine-tuning on expert data.\\n\\nLow Latency Critical: Fine-tuned models don't need retrieval steps, reducing latency. This \\nmatters for real-time applications.\\n\\nProprietary Knowledge: When your competitive advantage comes from specialized knowledge \\nembedded in the model itself.\\n\\nLimited Context Windows: If your use case requires more context than fits in a prompt, \\nfine-tuning can encode that knowledge in weights.\\n\\nThe Hybrid Approach\\n\\nMany successful applications combine both:\\n\\nFine-tune a base model on your domain to establish baseline knowledge and communication \\nstyle. Then use RAG on top for dynamic, factual information. This gives you the benefits \\nof both approaches.\\n\\nExample: A customer service bot might be fine-tuned on your company's communication style \\nand common workflows, while using RAG to retrieve current product specs, pricing, and \\npolicies.\\n\\nPractical Considerations\\n\\nData Requirements:\\n- RAG: Needs organized, retrievable documents. Quality matters more than quantity.\\n- Fine-tuning: Requires hundreds to thousands of high-quality training examples.\\n\\nCost Structure:\\n- RAG: Ongoing inference costs for embeddings and retrieval. Storage costs for vector DB.\\n- Fine-tuning: Upfront training costs. Potentially lower per-request costs afterward.\\n\\nMaintenance:\\n- RAG: Easy to update - just modify the document corpus.\\n- Fine-tuning: Requires retraining for updates, which can be expensive and time-consuming.\\n\\nComplexity:\\n- RAG: Simpler to implement initially but requires managing retrieval infrastructure.\\n- Fine-tuning: Requires ML expertise and training infrastructure.\\n\\nDecision Framework\\n\\nAsk yourself these questions:\\n\\n1. How often does the underlying information change?\\n2. Do you need source attribution?\\n3. What's your ML team's expertise level?\\n4. What's your budget for training vs. inference?\\n5. How critical is response latency?\\n6. Do you need consistent style/format?\\n7. How much training data do you have?\\n\\nCommon Mistakes\\n\\nDon't fine-tune when RAG would suffice: Fine-tuning is overkill for many use cases. Start \\nwith RAG unless you have specific reasons to fine-tune.\\n\\nNot considering hybrid approaches: Many assume it's either/or. Combining both often yields \\nthe best results.\\n\\nUnderestimating maintenance: Fine-tuned models need regular updates as information changes. \\nPlan for ongoing retraining.\\n\\nIgnoring evaluation: Rigorously test both approaches on your specific use case before \\ncommitting.\\n\\nGetting Started\\n\\nStart with RAG for most applications. It's faster to implement, easier to iterate on, and \\nprovides transparency. Once you've validated your use case and gathered production data, \\nconsider whether fine-tuning would provide meaningful improvements.\\n\\nIf you do fine-tune, start small. Fine-tune smaller models first to validate your approach \\nbefore investing in training larger models. Use parameter-efficient techniques like LoRA \\nto reduce costs.\\n\\nThe Future\\n\\nThe line between RAG and fine-tuning is blurring. Techniques like retrieval-enhanced \\nfine-tuning and continuous learning systems combine both approaches. As models improve and \\ntools mature, implementing hybrid solutions will become easier.\\n\\nChoose based on your specific requirements, not trends or hype. The best approach depends \\non your use case, resources, and constraints. Both RAG and fine-tuning are powerful tools \\n- understanding when to use each is key to building effective AI applications.\\n        \"),\n",
       " Document(metadata={'id': 'blog_006', 'title': 'LangChain Deep Dive: Building Complex AI Workflows', 'category': 'blog', 'author': 'Alex Thompson', 'website': \"Developer's Corner\", 'date': '2024-02-17', 'tags': ['LangChain', 'Python', 'AI Framework', 'Agents'], 'word_count': 1045, 'reading_time_minutes': 7, 'sentiment': 'positive', 'difficulty': 'intermediate'}, page_content='\\nLangChain has become the go-to framework for building sophisticated LLM applications. This \\ndeep dive explores its architecture, key components, and best practices for production use.\\n\\nWhat is LangChain?\\n\\nLangChain is an open-source framework for building applications powered by large language \\nmodels. It provides abstractions and tools for common patterns like prompt management, \\nchain composition, memory management, and agent creation. Think of it as the Rails or \\nDjango of LLM development - opinionated patterns that accelerate development.\\n\\nCore Concepts\\n\\nModels: LangChain supports multiple LLM providers (OpenAI, Anthropic, Hugging Face, etc.) \\nthrough a unified interface. Switch providers without rewriting code.\\n\\nPrompts: Reusable prompt templates with variable substitution. Version control your prompts \\nalongside code.\\n\\nChains: Sequence multiple LLM calls and operations. Output from one step becomes input to \\nthe next.\\n\\nAgents: LLMs that can use tools and make decisions about which actions to take based on \\nuser input.\\n\\nMemory: Persist state across interactions for conversational applications.\\n\\nBuilding Blocks\\n\\nPrompt Templates:\\n```python\\nfrom langchain import PromptTemplate\\n\\ntemplate = PromptTemplate(\\n    input_variables=[\"product\", \"features\"],\\n    template=\"Write a product description for {product} highlighting {features}\"\\n)\\n```\\n\\nTemplates ensure consistency and make prompts maintainable. Use them for all but the \\nsimplest one-off queries.\\n\\nChains:\\n\\nSimple chains connect operations linearly. Use LLMChain for basic prompt-to-response flows. \\nSequentialChain runs multiple chains in sequence, passing outputs as inputs. More complex \\napplications use custom chains with conditional logic.\\n\\nExample use case: A chain that summarizes a document, extracts key points, then generates \\nsocial media posts based on those points.\\n\\nAgents:\\n\\nAgents are the most powerful but complex LangChain feature. An agent uses an LLM to decide \\nwhich tools to use and in what order. Tools can be APIs, databases, calculators, search \\nengines, or custom functions.\\n\\nAgent types include:\\n- Zero-shot ReAct: Decides tool usage based on descriptions\\n- Conversational: Maintains dialogue context\\n- OpenAI Functions: Uses function calling for tool selection\\n- Plan-and-Execute: Creates plans before execution\\n\\nAgents enable building assistants that can accomplish complex, multi-step tasks \\nautonomously.\\n\\nMemory Management\\n\\nLLMs are stateless - they don\\'t remember previous interactions. LangChain provides memory \\ntypes to maintain context:\\n\\nConversationBufferMemory: Stores entire conversation history\\nConversationSummaryMemory: Summarizes old messages to save tokens\\nConversationBufferWindowMemory: Keeps only recent N interactions\\nVectorStoreMemory: Stores conversations in vector DB for semantic retrieval\\n\\nChoose based on your context window limits and use case requirements.\\n\\nRAG with LangChain\\n\\nLangChain simplifies RAG implementation:\\n\\n```python\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.chains import RetrievalQA\\n\\nvectorstore = Chroma.from_documents(documents, OpenAIEmbeddings())\\nqa = RetrievalQA.from_chain_type(\\n    llm=ChatOpenAI(),\\n    chain_type=\"stuff\",\\n    retriever=vectorstore.as_retriever()\\n)\\n```\\n\\nThis pattern works for most RAG applications. Customize retrieval parameters and chain \\ntypes for specific needs.\\n\\nProduction Considerations\\n\\nCaching: LangChain supports caching to reduce costs and improve latency. Enable for \\nrepeated queries.\\n\\nCallbacks: Hook into chain execution for logging, monitoring, and debugging. Essential for \\nproduction observability.\\n\\nError Handling: LLMs can fail unexpectedly. Implement retry logic and fallbacks. LangChain \\nprovides utilities for this.\\n\\nToken Management: Monitor and limit token usage to control costs. Use the token counting \\nutilities.\\n\\nAsync Support: For high-throughput applications, use LangChain\\'s async interfaces to \\nimprove performance.\\n\\nCommon Pitfalls\\n\\nOver-chaining: Don\\'t create complex chains when simple prompts suffice. Each chain step \\nadds latency and potential failure points.\\n\\nIgnoring Costs: Complex chains can consume many tokens quickly. Monitor usage carefully.\\n\\nNot Testing Individual Components: Test each chain component independently before \\ncomposing them.\\n\\nPremature Agent Usage: Agents are powerful but add complexity and unpredictability. Start \\nwith simpler approaches.\\n\\nAlternatives and Comparisons\\n\\nWhile LangChain is popular, consider alternatives:\\n\\nLlamaIndex: Better for RAG-focused applications with sophisticated retrieval needs.\\n\\nSemantic Kernel: Microsoft\\'s framework with strong .NET support.\\n\\nHaystack: Focuses on NLP pipelines and document processing.\\n\\nCustom Solutions: For simple use cases, direct API calls might be clearer than framework \\nabstractions.\\n\\nChoose based on your team\\'s expertise, use case requirements, and ecosystem preferences.\\n\\nAdvanced Patterns\\n\\nSelf-Ask with Search: Agent asks itself clarifying questions and searches for answers.\\n\\nConstitutional AI: Define principles that guide agent behavior and decision-making.\\n\\nMulti-Agent Systems: Multiple specialized agents collaborate on complex tasks.\\n\\nHuman-in-the-Loop: Pause execution for human review before critical actions.\\n\\nGetting Started\\n\\nBegin with simple chains before moving to complex agents. LangChain\\'s abstractions make \\nsense once you understand basic LLM patterns. If you\\'re new to LLMs, build a few simple \\napplications with direct API calls first.\\n\\nThe documentation is comprehensive but can be overwhelming. Start with the quickstart \\nguides and build progressively more complex applications.\\n\\nThe Future\\n\\nLangChain evolves rapidly, with new features and improvements released frequently. The \\nframework is moving toward better production-readiness with improved monitoring, evaluation \\ntools, and deployment patterns.\\n\\nLangSmith, the companion platform, provides hosted services for prompt management, \\nevaluation, and monitoring. Consider it for production applications.\\n\\nLangChain accelerates LLM application development but isn\\'t magic. It\\'s a tool that \\nrequires understanding both its abstractions and the underlying LLM concepts. Master both \\nto build powerful, maintainable AI applications.\\n        ')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_posts_doc = []\n",
    "for blog in blog_posts:\n",
    "    doc = Document(\n",
    "        page_content=blog['content'],\n",
    "        metadata={\n",
    "            'id': blog['id'],\n",
    "            'title': blog['title'],\n",
    "            'category': blog['category'],\n",
    "            'author': blog['author'],\n",
    "            'website': blog['website'],\n",
    "            'date': blog['date'],\n",
    "            'tags': blog['tags'],\n",
    "            'word_count': blog['word_count'],\n",
    "            'reading_time_minutes': blog['reading_time_minutes'],\n",
    "            'sentiment': blog['sentiment'],\n",
    "            'difficulty': blog['difficulty']\n",
    "        }\n",
    "    )\n",
    "    blog_posts_doc.append(doc)\n",
    "blog_posts_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fa0f753",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=350,\n",
    "    length_function=len,\n",
    "    separators=['\\n\\n','\\n',' ', '']\n",
    ")\n",
    "news_article_chunks = text_splitter.split_documents(news_articles_doc)\n",
    "tweets_chunks = text_splitter.split_documents(tweets_doc)\n",
    "blog_posts_chunks = text_splitter.split_documents(blog_posts_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3daeff22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of news articles: 5\n",
      "Number of news article chunks: 10\n",
      "Number of tweets: 8\n",
      "Number of tweet chunks: 8\n",
      "Number of blog posts: 6\n",
      "Number of blog post chunks: 43\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of news articles: {len(news_articles_doc)}\")\n",
    "print(f\"Number of news article chunks: {len(news_article_chunks)}\")\n",
    "print(f\"Number of tweets: {len(tweets_doc)}\")\n",
    "print(f\"Number of tweet chunks: {len(tweets_chunks)}\")\n",
    "print(f\"Number of blog posts: {len(blog_posts_doc)}\")\n",
    "print(f\"Number of blog post chunks: {len(blog_posts_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4935696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 61 documents to Pinecone index 'rag-udemy'\n"
     ]
    }
   ],
   "source": [
    "vector_store = PineconeVectorStore.from_documents(\n",
    "    documents=news_article_chunks + tweets_chunks + blog_posts_chunks,\n",
    "    embedding=embeddings,\n",
    "    index_name=INDEX_NAME\n",
    ")\n",
    "print(f\"Added {len(news_article_chunks) + len(tweets_chunks) + len(blog_posts_chunks)} documents to Pinecone index '{INDEX_NAME}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1132782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['PineconeVectorStore', 'OpenAIEmbeddings'], vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x16acedd10>, search_kwargs={'k': 3})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create retriever\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}  # Retrieve top 3 similar chunks\n",
    ")\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5846d230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 32768, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x169b9b4d0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x300160e90>, model_name='llama-3.3-70b-versatile', temperature=1e-08, model_kwargs={}, groq_api_key=SecretStr('**********'), max_tokens=1024)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initlialize LLM\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0,\n",
    "    max_tokens=1024\n",
    ")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bae30b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='You are a helpful AI assistant. Answer the question based on the provided context.\\nIf you cannot find the answer in the context, say \"I don\\'t have enough information to answer that question.\"\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\nAnswer:'), additional_kwargs={})])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a prompt template\n",
    "prompt_template = \"\"\"You are a helpful AI assistant. Answer the question based on the provided context.\n",
    "If you cannot find the answer in the context, say \"I don't have enough information to answer that question.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2728734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: VectorStoreRetriever(tags=['PineconeVectorStore', 'OpenAIEmbeddings'], vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x16acedd10>, search_kwargs={'k': 3})\n",
       "           | RunnableLambda(format_docs),\n",
       "  question: RunnablePassthrough()\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='You are a helpful AI assistant. Answer the question based on the provided context.\\nIf you cannot find the answer in the context, say \"I don\\'t have enough information to answer that question.\"\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\nAnswer:'), additional_kwargs={})])\n",
       "| ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 32768, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x169b9b4d0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x300160e90>, model_name='llama-3.3-70b-versatile', temperature=1e-08, model_kwargs={}, groq_api_key=SecretStr('**********'), max_tokens=1024)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building RAG chain\n",
    "def format_docs(docs):\n",
    "    \"\"\" Format retrieved documents for context \"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\" : RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37466ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "\n",
      "Query: What is RAG?\n",
      "\n",
      "Number of context chunks retrieved: 3\n",
      "\n",
      "Context Chunk 1:\n",
      "The Future of RAG\n",
      "\n",
      "As RAG systems mature, we're seeing exciting developments:\n",
      "- Agentic RAG with iterative retrieval\n",
      "- Multi-modal RAG combining text, images, and structured data\n",
      "- Conversational RAG maintaining context across turns\n",
      "- Self-RAG where models evaluate and refine retrievals\n",
      "\n",
      "RAG is not just a temporary solution - it represents a fundamental approach to building \n",
      "reliable, maintainable AI systems. Whether you're building a customer support chatbot, \n",
      "a research assistant, or an enterprise knowledge base, RAG should be in your toolkit.\n",
      "\n",
      "Context Chunk 2:\n",
      "The RAG Architecture\n",
      "\n",
      "A typical RAG system consists of three main components:\n",
      "\n",
      "1. Document Processing: Documents are split into chunks, converted into embeddings using \n",
      "models like OpenAI's text-embedding-3-small, and stored in a vector database such as \n",
      "Pinecone, Weaviate, or Chroma.\n",
      "\n",
      "2. Retrieval: When a user asks a question, the query is embedded and used to find the \n",
      "most relevant document chunks through semantic similarity search.\n",
      "\n",
      "3. Generation: The retrieved context is combined with the user's query and sent to a \n",
      "large language model (like GPT-4, Claude, or Llama 2) to generate a grounded response.\n",
      "\n",
      "Why RAG Matters\n",
      "\n",
      "RAG addresses several key limitations of standalone LLMs:\n",
      "- Reduces hallucinations by grounding responses in factual sources\n",
      "- Enables access to current information without retraining\n",
      "- Allows for domain-specific knowledge without fine-tuning\n",
      "- Provides transparency through source attribution\n",
      "- More cost-effective than constantly retraining models\n",
      "\n",
      "Context Chunk 3:\n",
      "Prompt Engineering: Include clear instructions for using retrieved context. Ask the model \n",
      "to cite sources and admit when information is insufficient. Structure prompts to minimize \n",
      "hallucination.\n",
      "\n",
      "Evaluation: Measure both retrieval quality (recall, precision) and generation quality \n",
      "(accuracy, relevance, coherence). Use human evaluation alongside automated metrics.\n",
      "\n",
      "Common Pitfalls to Avoid\n",
      "\n",
      "1. Poor chunking leading to incomplete context\n",
      "2. Using too many or too few retrieved chunks\n",
      "3. Not handling retrieval failures gracefully\n",
      "4. Ignoring latency optimization\n",
      "5. Insufficient testing on edge cases\n",
      "\n",
      "The Future of RAG\n",
      "\n",
      "As RAG systems mature, we're seeing exciting developments:\n",
      "- Agentic RAG with iterative retrieval\n",
      "- Multi-modal RAG combining text, images, and structured data\n",
      "- Conversational RAG maintaining context across turns\n",
      "- Self-RAG where models evaluate and refine retrievals\n",
      "\n",
      "Response: RAG stands for Retrieval-Augmented Generation. It is a fundamental approach to building reliable, maintainable AI systems, consisting of three main components: Document Processing, Retrieval, and Generation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test RAG Chain\n",
    "query = \"What is RAG?\"\n",
    "response = rag_chain.invoke(query)\n",
    "context = retriever.invoke(query)\n",
    "print(f\"\\n\"+\"=\" * 60 + \"\\n\")\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Number of context chunks retrieved: {len(context)}\\n\")\n",
    "for i, context_chunk in enumerate(context):\n",
    "    print(f\"Context Chunk {i+1}:\\n{context_chunk.page_content}\\n\")\n",
    "print(f\"Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6307fecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "\n",
      "BATCH PROCESSING RESULTS:\n",
      "\n",
      "============================================================\n",
      "\n",
      "Query: What are the benefits of using RAG in AI applications?\n",
      "\n",
      "Number of context chunks retrieved: 3\n",
      "\n",
      "Context Chunk 1:\n",
      "The Future of RAG\n",
      "\n",
      "As RAG systems mature, we're seeing exciting developments:\n",
      "- Agentic RAG with iterative retrieval\n",
      "- Multi-modal RAG combining text, images, and structured data\n",
      "- Conversational RAG maintaining context across turns\n",
      "- Self-RAG where models evaluate and refine retrievals\n",
      "\n",
      "RAG is not just a temporary solution - it represents a fundamental approach to building \n",
      "reliable, maintainable AI systems. Whether you're building a customer support chatbot, \n",
      "a research assistant, or an enterprise knowledge base, RAG should be in your toolkit.\n",
      "\n",
      "Context Chunk 2:\n",
      "Retrieval-Augmented Generation (RAG) has emerged as one of the most practical and powerful \n",
      "techniques for building AI applications. In this comprehensive guide, we'll explore what \n",
      "RAG is, why it matters, and how to implement it effectively.\n",
      "\n",
      "What is RAG?\n",
      "\n",
      "RAG combines the power of large language models with external knowledge retrieval. Instead \n",
      "of relying solely on the information encoded in the model's parameters during training, \n",
      "RAG systems retrieve relevant information from a knowledge base and use it to generate \n",
      "more accurate, up-to-date, and contextually appropriate responses.\n",
      "\n",
      "The RAG Architecture\n",
      "\n",
      "A typical RAG system consists of three main components:\n",
      "\n",
      "1. Document Processing: Documents are split into chunks, converted into embeddings using \n",
      "models like OpenAI's text-embedding-3-small, and stored in a vector database such as \n",
      "Pinecone, Weaviate, or Chroma.\n",
      "\n",
      "Context Chunk 3:\n",
      "The RAG Architecture\n",
      "\n",
      "A typical RAG system consists of three main components:\n",
      "\n",
      "1. Document Processing: Documents are split into chunks, converted into embeddings using \n",
      "models like OpenAI's text-embedding-3-small, and stored in a vector database such as \n",
      "Pinecone, Weaviate, or Chroma.\n",
      "\n",
      "2. Retrieval: When a user asks a question, the query is embedded and used to find the \n",
      "most relevant document chunks through semantic similarity search.\n",
      "\n",
      "3. Generation: The retrieved context is combined with the user's query and sent to a \n",
      "large language model (like GPT-4, Claude, or Llama 2) to generate a grounded response.\n",
      "\n",
      "Why RAG Matters\n",
      "\n",
      "RAG addresses several key limitations of standalone LLMs:\n",
      "- Reduces hallucinations by grounding responses in factual sources\n",
      "- Enables access to current information without retraining\n",
      "- Allows for domain-specific knowledge without fine-tuning\n",
      "- Provides transparency through source attribution\n",
      "- More cost-effective than constantly retraining models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Response: The benefits of using RAG in AI applications include:\n",
      "\n",
      "1. Reduces hallucinations by grounding responses in factual sources\n",
      "2. Enables access to current information without retraining\n",
      "3. Allows for domain-specific knowledge without fine-tuning\n",
      "4. Provides transparency through source attribution\n",
      "5. Is more cost-effective than constantly retraining models\n",
      "\n",
      "These benefits make RAG a valuable approach for building reliable, maintainable AI systems, such as customer support chatbots, research assistants, or enterprise knowledge bases.\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Query: Explain the key features of vector databases.\n",
      "\n",
      "Number of context chunks retrieved: 3\n",
      "\n",
      "Context Chunk 1:\n",
      "The vector database market is rapidly evolving, with new features and optimizations \n",
      "appearing regularly. As AI applications become more sophisticated, vector databases will \n",
      "play an increasingly central role in the infrastructure stack.\n",
      "\n",
      "Context Chunk 2:\n",
      "Vector databases have become essential infrastructure for AI applications. This guide \n",
      "covers everything you need to know about vector databases, from basic concepts to \n",
      "production deployment.\n",
      "\n",
      "What Are Vector Databases?\n",
      "\n",
      "Vector databases are specialized systems designed to store, index, and query high-dimensional \n",
      "vector embeddings. Unlike traditional databases that store and query structured data, vector \n",
      "databases excel at semantic similarity search, finding items that are conceptually similar \n",
      "even if they don't share exact keywords.\n",
      "\n",
      "How Vector Embeddings Work\n",
      "\n",
      "Embeddings are dense numerical representations of data (text, images, audio) that capture \n",
      "semantic meaning. Similar items have similar embeddings, which can be measured using distance \n",
      "metrics like cosine similarity, Euclidean distance, or dot product. For example, the \n",
      "embeddings for \"king\" and \"monarch\" would be very close in vector space, while \"king\" and \n",
      "\"banana\" would be distant.\n",
      "\n",
      "Key Features of Vector Databases\n",
      "\n",
      "Context Chunk 3:\n",
      "Key Features of Vector Databases\n",
      "\n",
      "Modern vector databases provide several critical capabilities:\n",
      "\n",
      "Approximate Nearest Neighbor (ANN) Search: Efficiently find similar vectors among millions \n",
      "or billions of entries using algorithms like HNSW (Hierarchical Navigable Small World) or \n",
      "IVF (Inverted File Index).\n",
      "\n",
      "Metadata Filtering: Combine semantic search with traditional filtering, like finding similar \n",
      "documents from a specific date range or category.\n",
      "\n",
      "Scalability: Handle massive datasets with horizontal scaling and distributed architectures.\n",
      "\n",
      "Real-time Updates: Support continuous ingestion and immediate availability of new vectors.\n",
      "\n",
      "Hybrid Search: Combine vector similarity with keyword search and business logic.\n",
      "\n",
      "Popular Vector Databases\n",
      "\n",
      "Pinecone: Fully managed, serverless vector database with excellent performance and ease of \n",
      "use. Great for production applications requiring high availability.\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Response: The key features of vector databases include:\n",
      "\n",
      "1. Approximate Nearest Neighbor (ANN) Search: This allows for efficient searching of similar vectors among millions or billions of entries using algorithms like HNSW or IVF.\n",
      "\n",
      "2. Metadata Filtering: This feature combines semantic search with traditional filtering, enabling users to find similar documents from a specific date range or category.\n",
      "\n",
      "3. Scalability: Vector databases can handle massive datasets through horizontal scaling and distributed architectures.\n",
      "\n",
      "4. Real-time Updates: They support continuous ingestion and immediate availability of new vectors.\n",
      "\n",
      "5. Hybrid Search: This feature combines vector similarity with keyword search and business logic, allowing for more flexible and powerful search capabilities.\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Query: What are some best practices for prompt engineering?\n",
      "\n",
      "Number of context chunks retrieved: 3\n",
      "\n",
      "Context Chunk 1:\n",
      "The Future of Prompting\n",
      "\n",
      "As models become more capable, prompt engineering is evolving from an art to an engineering \n",
      "discipline. Expect to see:\n",
      "- Automated prompt optimization\n",
      "- Standard libraries of proven prompts\n",
      "- Better tools for testing and validation\n",
      "- Integration with software development practices\n",
      "\n",
      "Mastering prompt engineering multiplies your effectiveness with AI systems. Invest time in \n",
      "learning these techniques - it's one of the highest-leverage skills in modern technology.\n",
      "\n",
      "Context Chunk 2:\n",
      "Common Mistakes to Avoid\n",
      "\n",
      "1. Vague instructions leading to inconsistent outputs\n",
      "2. Overloading prompts with too many requirements\n",
      "3. Not providing sufficient context\n",
      "4. Failing to specify output format\n",
      "5. Not testing with edge cases\n",
      "6. Ignoring token limits and costs\n",
      "\n",
      "Tools and Frameworks\n",
      "\n",
      "Modern prompt engineering benefits from frameworks like:\n",
      "- LangChain for complex prompt chains and workflows\n",
      "- Guidance for structured output generation\n",
      "- OpenAI's Playground for rapid experimentation\n",
      "- Custom testing harnesses for evaluation\n",
      "\n",
      "The Future of Prompting\n",
      "\n",
      "As models become more capable, prompt engineering is evolving from an art to an engineering \n",
      "discipline. Expect to see:\n",
      "- Automated prompt optimization\n",
      "- Standard libraries of proven prompts\n",
      "- Better tools for testing and validation\n",
      "- Integration with software development practices\n",
      "\n",
      "Context Chunk 3:\n",
      "Iterative Refinement\n",
      "\n",
      "Treat prompt engineering as an iterative process:\n",
      "\n",
      "1. Start with a basic prompt\n",
      "2. Analyze the output for issues\n",
      "3. Refine the prompt to address specific problems\n",
      "4. Test with multiple inputs\n",
      "5. Document what works\n",
      "\n",
      "Advanced Techniques\n",
      "\n",
      "Self-Consistency: Generate multiple responses and aggregate them for more reliable results \n",
      "on critical tasks.\n",
      "\n",
      "Constitutional AI: Define explicit principles and values for the model to follow, creating \n",
      "ethical and aligned outputs.\n",
      "\n",
      "Prompt Chaining: Break complex tasks into subtasks, using the output of one prompt as input \n",
      "to the next.\n",
      "\n",
      "Meta-Prompting: Have the model help optimize its own prompts through iterative feedback.\n",
      "\n",
      "Common Mistakes to Avoid\n",
      "\n",
      "1. Vague instructions leading to inconsistent outputs\n",
      "2. Overloading prompts with too many requirements\n",
      "3. Not providing sufficient context\n",
      "4. Failing to specify output format\n",
      "5. Not testing with edge cases\n",
      "6. Ignoring token limits and costs\n",
      "\n",
      "Tools and Frameworks\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Response: Some best practices for prompt engineering include:\n",
      "\n",
      "1. Treating prompt engineering as an iterative process:\n",
      "   - Start with a basic prompt\n",
      "   - Analyze the output for issues\n",
      "   - Refine the prompt to address specific problems\n",
      "   - Test with multiple inputs\n",
      "   - Document what works\n",
      "\n",
      "2. Avoiding common mistakes:\n",
      "   - Vague instructions leading to inconsistent outputs\n",
      "   - Overloading prompts with too many requirements\n",
      "   - Not providing sufficient context\n",
      "   - Failing to specify output format\n",
      "   - Not testing with edge cases\n",
      "   - Ignoring token limits and costs\n",
      "\n",
      "3. Utilizing advanced techniques:\n",
      "   - Self-Consistency: Generate multiple responses and aggregate them for more reliable results\n",
      "   - Constitutional AI: Define explicit principles and values for the model to follow\n",
      "   - Prompt Chaining: Break complex tasks into subtasks\n",
      "   - Meta-Prompting: Have the model help optimize its own prompts through iterative feedback\n",
      "\n",
      "4. Leveraging tools and frameworks:\n",
      "   - LangChain for complex prompt chains and workflows\n",
      "   - Guidance for structured output generation\n",
      "   - OpenAI's Playground for rapid experimentation\n",
      "   - Custom testing harnesses for evaluation\n",
      "\n",
      "5. Investing time in learning prompt engineering techniques, as it is a high-leverage skill in modern technology.\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Batch queries\n",
    "queries = [\n",
    "    \"What are the benefits of using RAG in AI applications?\",\n",
    "    \"Explain the key features of vector databases.\",\n",
    "    \"What are some best practices for prompt engineering?\"\n",
    "]\n",
    "\n",
    "contexts = retriever.batch(queries)\n",
    "responses = rag_chain.batch(queries)\n",
    "\n",
    "# Display results\n",
    "results = {\n",
    "    query: {\"context\": context, \"response\": response }\n",
    "    for query, context, response in zip(queries, contexts, responses)\n",
    "}\n",
    "\n",
    "print(f\"\\n\"+\"=\" * 60 + \"\\n\")\n",
    "print(\"BATCH PROCESSING RESULTS:\\n\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "for query, result in results.items():\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print(f\"Number of context chunks retrieved: {len(result['context'])}\\n\")\n",
    "    for i, context_chunk in enumerate(result['context']):\n",
    "        print(f\"Context Chunk {i+1}:\\n{context_chunk.page_content}\\n\")\n",
    "    print(\"-\" * 60 + \"\\n\")\n",
    "    print(f\"Response: {result['response']}\\n\")\n",
    "    print(\"-\" * 60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b8ff882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "\n",
      "SIMILARITY SEARCH RESULTS:\n",
      "============================================================\n",
      "\n",
      "Result 1:\n",
      "The Future of RAG\n",
      "\n",
      "As RAG systems mature, we're seeing exciting developments:\n",
      "- Agentic RAG with iterative retrieval\n",
      "- Multi-modal RAG combining text, images, and structured data\n",
      "- Conversational RAG \n",
      "Metadata: {'author': 'Sarah Chen', 'category': 'blog', 'date': '2024-01-12', 'difficulty': 'intermediate', 'id': 'blog_001', 'reading_time_minutes': 5.0, 'sentiment': 'positive', 'tags': ['RAG', 'LLM', 'Vector Database', 'AI Engineering'], 'title': 'Understanding RAG: The Future of AI-Powered Applications', 'website': 'AI Engineering Blog', 'word_count': 847.0}\n",
      "\n",
      "Result 2:\n",
      "The RAG Architecture\n",
      "\n",
      "A typical RAG system consists of three main components:\n",
      "\n",
      "1. Document Processing: Documents are split into chunks, converted into embeddings using \n",
      "models like OpenAI's text-embed\n",
      "Metadata: {'author': 'Sarah Chen', 'category': 'blog', 'date': '2024-01-12', 'difficulty': 'intermediate', 'id': 'blog_001', 'reading_time_minutes': 5.0, 'sentiment': 'positive', 'tags': ['RAG', 'LLM', 'Vector Database', 'AI Engineering'], 'title': 'Understanding RAG: The Future of AI-Powered Applications', 'website': 'AI Engineering Blog', 'word_count': 847.0}\n",
      "\n",
      "Result 3:\n",
      "Prompt Engineering: Include clear instructions for using retrieved context. Ask the model \n",
      "to cite sources and admit when information is insufficient. Structure prompts to minimize \n",
      "hallucination.\n",
      "\n",
      "Ev\n",
      "Metadata: {'author': 'Sarah Chen', 'category': 'blog', 'date': '2024-01-12', 'difficulty': 'intermediate', 'id': 'blog_001', 'reading_time_minutes': 5.0, 'sentiment': 'positive', 'tags': ['RAG', 'LLM', 'Vector Database', 'AI Engineering'], 'title': 'Understanding RAG: The Future of AI-Powered Applications', 'website': 'AI Engineering Blog', 'word_count': 847.0}\n",
      "\n",
      "============================================================\n",
      "\n",
      "SIMILARITY SEARCH WITH SCORES RESULTS:\n",
      "============================================================\n",
      "Scores: 0.6521\n",
      "Content: The Future of RAG\n",
      "\n",
      "As RAG systems mature, we're seeing exciting developments:\n",
      "- Agentic RAG with iterative retrieval\n",
      "- Multi-modal RAG combining text, images, and structured data\n",
      "- Conversational RAG \n",
      "Metadata: {'author': 'Sarah Chen', 'category': 'blog', 'date': '2024-01-12', 'difficulty': 'intermediate', 'id': 'blog_001', 'reading_time_minutes': 5.0, 'sentiment': 'positive', 'tags': ['RAG', 'LLM', 'Vector Database', 'AI Engineering'], 'title': 'Understanding RAG: The Future of AI-Powered Applications', 'website': 'AI Engineering Blog', 'word_count': 847.0}\n",
      "Scores: 0.6045\n",
      "Content: The RAG Architecture\n",
      "\n",
      "A typical RAG system consists of three main components:\n",
      "\n",
      "1. Document Processing: Documents are split into chunks, converted into embeddings using \n",
      "models like OpenAI's text-embed\n",
      "Metadata: {'author': 'Sarah Chen', 'category': 'blog', 'date': '2024-01-12', 'difficulty': 'intermediate', 'id': 'blog_001', 'reading_time_minutes': 5.0, 'sentiment': 'positive', 'tags': ['RAG', 'LLM', 'Vector Database', 'AI Engineering'], 'title': 'Understanding RAG: The Future of AI-Powered Applications', 'website': 'AI Engineering Blog', 'word_count': 847.0}\n",
      "Scores: 0.5597\n",
      "Content: Prompt Engineering: Include clear instructions for using retrieved context. Ask the model \n",
      "to cite sources and admit when information is insufficient. Structure prompts to minimize \n",
      "hallucination.\n",
      "\n",
      "Ev\n",
      "Metadata: {'author': 'Sarah Chen', 'category': 'blog', 'date': '2024-01-12', 'difficulty': 'intermediate', 'id': 'blog_001', 'reading_time_minutes': 5.0, 'sentiment': 'positive', 'tags': ['RAG', 'LLM', 'Vector Database', 'AI Engineering'], 'title': 'Understanding RAG: The Future of AI-Powered Applications', 'website': 'AI Engineering Blog', 'word_count': 847.0}\n"
     ]
    }
   ],
   "source": [
    "# Similarity search\n",
    "\n",
    "### Direct similarity search\n",
    "search_results = vector_store.similarity_search(\n",
    "    \"What is RAG?\", k=3\n",
    ")\n",
    "print(f\"\\n{'='*60}\\n\")\n",
    "print(\"SIMILARITY SEARCH RESULTS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, doc in enumerate(search_results):\n",
    "    print(f\"\\nResult {i+1}:\")\n",
    "    print(doc.page_content[:200])\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "\n",
    "### Similarity search with scores\n",
    "search_results_with_scores = vector_store.similarity_search_with_score(\n",
    "    \"What is RAG?\", k=3\n",
    ")\n",
    "print(f\"\\n{'='*60}\\n\")\n",
    "print(\"SIMILARITY SEARCH WITH SCORES RESULTS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for doc, score in search_results_with_scores:\n",
    "    print(f\"Scores: {score:.4f}\")\n",
    "    print(f\"Content: {doc.page_content[:200]}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b3f0815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FILTERED SEARCH RESULTS\n",
      "============================================================\n",
      "\n",
      "\n",
      "Content: OpenAI has announced the release of GPT-4 Turbo, an improved version of their flagship \n",
      "language model. The new model features a larger context window of 128,000 tokens, \n",
      "significantly improved performance on complex reasoning tasks, and reduced pricing. \n",
      "GPT-4 Turbo can process the equivalent of approximately 300 pages of text in a single \n",
      "prompt, making it ideal for analyzing lengthy documents, codebases, and research papers.\n",
      "\n",
      "The model also includes improved instruction following and more consistent output formatting. \n",
      "OpenAI has reported that GPT-4 Turbo is 40% more accurate on factual questions compared to \n",
      "its predecessor and shows reduced hallucination rates. The API pricing has been reduced by \n",
      "50% for input tokens and 33% for output tokens, making it more accessible for developers \n",
      "and businesses.\n",
      "\n",
      "Metadata: {'category': 'news', 'date': '2024-01-15', 'id': 'news_001', 'source': 'TechCrunch', 'title': 'OpenAI Releases GPT-4 Turbo with Enhanced Capabilities'}\n",
      "\n",
      "Content: Key features include enhanced multimodal capabilities, better performance on mathematical \n",
      "problems, and improved code generation. The model is now available through the OpenAI API \n",
      "for enterprise customers and developers.\n",
      "\n",
      "Metadata: {'category': 'news', 'date': '2024-01-15', 'id': 'news_001', 'source': 'TechCrunch', 'title': 'OpenAI Releases GPT-4 Turbo with Enhanced Capabilities'}\n"
     ]
    }
   ],
   "source": [
    "# Advanced retrieval - Filtering\n",
    "\n",
    "filtered_results = vector_store.similarity_search(\n",
    "    'GPT-4 Turbo',\n",
    "    k=3,\n",
    "    filter={'source':{\"$eq\": 'TechCrunch'}}\n",
    ")\n",
    "print(f\"\\n\"+\"=\"*60)\n",
    "print(f\"FILTERED SEARCH RESULTS\")\n",
    "print(\"=\"*60+\"\\n\")\n",
    "for doc in filtered_results:\n",
    "    print(f\"\\nContent: {doc.page_content}\")\n",
    "    print(f\"\\nMetadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "469ab715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def create_rag_chain_from_existing_index():\n",
    "    \"\"\" Loading existing Pinecone index \"\"\"\n",
    "    vectorstore = PineConeVectorStore(\n",
    "        index_name=INDEX_NAME,\n",
    "        embedding=embeddings\n",
    "    )\n",
    "\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={'k':3})\n",
    "\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": retriever | format_docs,\n",
    "            \"question\": RunnablePassthrough()\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return rag_chain\n",
    "\n",
    "def add_data(new_docs):\n",
    "        \"\"\" Add more data to vector store \"\"\"\n",
    "        vectorstore = PineConeVectorStore(\n",
    "            index_name=INDEX_NAME,\n",
    "            embedding=embeddings,\n",
    "        )\n",
    "        # Split documents into chunks\n",
    "        text_splitter=RecursiveCharacterTextSplitter.from_documents(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=350,\n",
    "        )\n",
    "        new_doc_chunks = text_splitter.split_documents(new_docs)\n",
    "\n",
    "        # Add to vectostore\n",
    "        vectorstore.add_documents(new_doc_chunks)\n",
    "        print(f\"Added {len(new_doc_chunks)} chunks to the vector store {INDEX_NAME}\")\n",
    "    \n",
    "def get_index_stats():\n",
    "        \"\"\" Get statistics about the PineCone Index \"\"\"\n",
    "        index = pc.Index(INDEX_NAME)\n",
    "        stats = index.describe_index_stats()\n",
    "        print(f\"\\nIndex statistics:\")\n",
    "        print(f\" Total vectos: {stats['total_vector_count']}\")\n",
    "        print(f\" Dimension: {stats['dimension']}\")\n",
    "        print(f\" Index fullness: {stats.get('index_fullness', 'N/A')}\")\n",
    "        return stats\n",
    "    \n",
    "def delete_index():\n",
    "        \"\"\" Delete the Pinecone Index \"\"\"\n",
    "        pc.delete_index(INDEX_NAME)\n",
    "        print(f\"Index {INDEX_NAME} is deleted! \")\n",
    "    \n",
    "def clear_index():\n",
    "        \"\"\" Clear the Pinecone Index \"\"\"\n",
    "        index = pc.Index(INDEX_NAME)\n",
    "        index.delete(delete_all=True)\n",
    "        print(f\"All vectors in the index {INDEX_NAME} are cleared \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "119d2821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST RAG CHAIN\n",
      "============================================================\n",
      "\n",
      "\n",
      "Query: What is GPT-4 Turbo?\n",
      "Answer: GPT-4 Turbo is an improved version of OpenAI's flagship language model, featuring a larger context window, significantly improved performance on complex reasoning tasks, and reduced pricing. It can process approximately 300 pages of text in a single prompt and has improved instruction following, output formatting, and accuracy, with a 40% increase in accuracy on factual questions and reduced hallucination rates compared to its predecessor.\n",
      "\n",
      "Index statistics:\n",
      " Total vectos: 61\n",
      " Dimension: 1024\n",
      " Index fullness: 0.0\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    # Test single query\n",
    "    print(\"\\n\"+\"=\"*60)\n",
    "    print(\"TEST RAG CHAIN\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    query = \"What is GPT-4 Turbo?\"\n",
    "    result = rag_chain.invoke(query)\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"Answer: {result}\")\n",
    "    get_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367aec53",
   "metadata": {},
   "source": [
    "#### PineCone features:\n",
    "- Serverless Vector database\n",
    "- Ultra-fast similarity search\n",
    "- Real-time updates\n",
    "- Metadata filtering\n",
    "- Horizontal scaling\n",
    "- Low latency (p95 < 100ms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea8e073",
   "metadata": {},
   "source": [
    "#### GROQ features:\n",
    "- Ultra-fast inference (500+ tokens/sec)\n",
    "- Cost-effective\n",
    "- Multiple open-source models\n",
    "- High throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf37cb05",
   "metadata": {},
   "source": [
    "#### Embedding Dimensions:\n",
    "- OpenAI text-embedding-3-small: 1536\n",
    "- OpenAI text-embedding-ada-002: 1536\n",
    "- HuggingFace all-MiniLM-L6-v2: 384\n",
    "- HuggingFace all-mpnet-base-v2: 768"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91427d9",
   "metadata": {},
   "source": [
    "#### Note: Dimension must match between embeddings and PineCone index!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3266fba8",
   "metadata": {},
   "source": [
    "#### Search Types:\n",
    "- Similarity: Basic similarity search\n",
    "- mmr: Maximum Marginal Relevance (diverse results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d743f6e0",
   "metadata": {},
   "source": [
    "#### Metrics:\n",
    "- cosine: Good for text (default)\n",
    "- euclidean: Good for images/audio\n",
    "- dotproduct: Fast but requires normalized vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a4089d",
   "metadata": {},
   "source": [
    "#### Best Practices:\n",
    "1. Use serverless for variable workloads\n",
    "2. Use cosine similarity for text embeddings\n",
    "3. Set appropriate chunk size (500-1500 tokens)\n",
    "4. Use metadata filtering for multi-tenant apps\n",
    "5. Monitor index stats regularly\n",
    "6. Use namespaces for data isolation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGUdemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
