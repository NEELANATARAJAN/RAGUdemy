{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ef1ae67",
   "metadata": {},
   "source": [
    "# ðŸ§  Query Decomposition\n",
    "\n",
    "Query decomposition is the process of taking a complex, multi-part question and breaking into simpler, atomic sub-questions that can each be retrieved and answered individually.\n",
    "\n",
    "#### Why use Query Decomposition?\n",
    "- Complex queries often involve multiple concepts\n",
    "- LLMs or retrievers may miss parts of the original question\n",
    "- It enables multi-hop reasoning (answering in steps)\n",
    "- Allows parallelism (especially multi-agent frameworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "352fe703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23ff4ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load documents\n",
    "loader=TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "docs=loader.load()\n",
    "\n",
    "splitter=RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks=splitter.split_documents(docs)\n",
    "\n",
    "embedding_model=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore=FAISS.from_documents(chunks,embedding_model)\n",
    "retriever=vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":4, \"lambda_mult\":0.7})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732df4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Initialize Groq LLM\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "llm=init_chat_model(model=\"groq:llama-3.1-8b-instant\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fdfc7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Query decomposition\n",
    "decomposition_prompt=PromptTemplate.from_template(\"\"\"\n",
    "You are an AI assistant. Decompose the following question into 2 to 4 similar sub-questions for better\n",
    "document retrieval.\n",
    "\n",
    "Question: \"{question}\"\n",
    "\n",
    "Sub-questions:\n",
    "\"\"\")\n",
    "decomposition_chain=decomposition_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc3b25ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To facilitate better document retrieval, I can break down the original question into the following sub-questions:\n",
      "\n",
      "1. **What is LangChain's memory architecture?** This sub-question focuses on understanding how LangChain utilizes memory, which will help in retrieving documents that discuss its memory-related features.\n",
      "\n",
      "2. **How do LangChain agents interact with the environment?** This sub-question will allow us to retrieve documents that discuss LangChain's agent capabilities, such as decision-making and execution.\n",
      "\n",
      "3. **What is CrewAI's memory architecture compared to LangChain?** This sub-question compares the memory architectures of LangChain and CrewAI, which might help in retrieving documents that highlight their respective design choices.\n",
      "\n",
      "4. **How do CrewAI agents interact with the environment in comparison to LangChain?** This sub-question will allow us to retrieve documents that compare the agent capabilities of LangChain and CrewAI.\n",
      "\n",
      "By breaking down the original question into these sub-questions, we can retrieve more targeted and relevant documents for comparison purposes.\n"
     ]
    }
   ],
   "source": [
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "decompostion_query=decomposition_chain.invoke({\"question\": query})\n",
    "print(decompostion_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6adcb344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: QA chain per sub-question\n",
    "qa_prompt=PromptTemplate.from_template(\"\"\" \n",
    "Use the context below to answer the question.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "qa_chain=create_stuff_documents_chain(llm=llm, prompt=qa_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2eb67fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Complete RAG pipeline logic\n",
    "def query_decomposition_rag_pipeline(user_query):\n",
    "    # Decompose the query\n",
    "    sub_qs_text = decomposition_chain.invoke({\"question\":user_query})\n",
    "    sub_questions=[q.strip(\"-*1234567890. \").strip() for q in sub_qs_text.split(\"\\n\") if q.strip()]\n",
    "\n",
    "    results=[]\n",
    "    for subq in sub_questions:\n",
    "        docs=retriever.invoke(subq)\n",
    "        result=qa_chain.invoke({\"input\":subq, \"context\":docs})\n",
    "        results.append(f\"Q: {subq}\\nA: {result}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11fa2385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final answer:\n",
      "\n",
      "Q: To better retrieve relevant documents, I can decompose the given question into the following sub-questions:\n",
      "A: It seems like the text is describing a tool called LangChain, which is used for semantic search and retrieval. Based on the provided context, it appears that you're looking to break down a question into sub-questions to better use LangChain for retrieval. \n",
      "\n",
      "However, the original text does not explicitly mention how to decompose a question into sub-questions. But in general, the following steps can be taken:\n",
      "\n",
      "1. Identify the main entities or keywords in the question. \n",
      "2. Break down each entity or keyword into its components or synonyms, as this can help in broadening the search scope and catching semantically similar content.\n",
      "3. Identify any specific requirements or constraints mentioned in the question, such as date ranges, locations, or specific domains.\n",
      "4. Consider any specific tasks or goals mentioned in the question, such as finding a definition, comparing different options, or making a recommendation.\n",
      "\n",
      "For example, if the question is 'What are the benefits of using LangChain for semantic search?', the sub-questions could be:\n",
      "\n",
      "- What does LangChain do?\n",
      "- What are the benefits of using LangChain?\n",
      "- How does LangChain enable semantic search?\n",
      "- What are the advantages of using LangChain over other tools?\n",
      "\n",
      "Keep in mind that these steps are general guidelines and may need to be tailored to the specific use case and question being asked.\n",
      "\n",
      "Q: What is LangChain's memory architecture and how does it function?\n",
      "A: LangChain's memory architecture is composed of memory modules, specifically designed to enhance the functionality of its LLMs. These memory modules include:\n",
      "\n",
      "1. ConversationBufferMemory: This module allows the LLM to maintain awareness of previous conversation turns. This means that the LLM can recall and consider the context of previous conversation steps when generating responses.\n",
      "\n",
      "2. ConversationSummaryMemory: This module is capable of summarizing long interactions to fit within token limits. This is particularly useful when dealing with lengthy conversations or when working within strict token limits.\n",
      "\n",
      "In essence, LangChain's memory architecture functions by enabling the LLM to retain and draw upon previous information, either by recalling specific conversation turns or by condensing lengthy interactions into concise summaries. This memory architecture facilitates more contextual and efficient LLM interactions, allowing for the creation of complex chains of LLM calls and conditionally executed steps.\n",
      "\n",
      "Q: This sub-question isolates the memory component of LangChain, allowing for more focused document retrieval on the topic of memory architecture\n",
      "A: There is no information in the provided context that directly addresses a memory component of LangChain or a focused document retrieval on the topic of memory architecture. \n",
      "\n",
      "The context mentions LangChain's integration with vector databases for semantic search within large document corpora, particularly in the context of Retrieval-Augmented Generation (RAG), but it does not provide specific information about LangChain's memory capabilities or how it handles document retrieval on a specific topic like memory architecture.\n",
      "\n",
      "Q: What types of agents are used in LangChain's framework?\n",
      "A: The context provided does not specify any particular type of agent that is used in LangChain's framework. However, it does mention that agents use Large Language Models (LLMs) to reason about which tool to call, what input to provide, and how to process the output. But it does not provide any specific information on agent types.\n",
      "\n",
      "Q: This sub-question targets the agent component of LangChain, enabling more precise document retrieval on the specific types of agents used\n",
      "A: Based on the provided context, the sub-question targeting the agent component of LangChain is likely:\n",
      "\n",
      "\"What types of agents does LangChain support, enabling more precise document retrieval for their specific capabilities?\" \n",
      "\n",
      "This question targets the agent component of LangChain and aims to gather information about the specific types of agents used in this system, which could lead to more precise document retrieval for their respective capabilities.\n",
      "\n",
      "Q: How does LangChain's memory and agent architecture differ from CrewAI's?\n",
      "A: Based on the provided context, here's a comparison of LangChain's memory and agent architecture with CrewAI's:\n",
      "\n",
      "1. **Memory Use**: \n",
      "   - LangChain agents use **context-aware memory** across steps, allowing them to dynamically retain and reuse information gathered during the execution of a task. This context-aware memory facilitates more sophisticated decision-making and adaptability in their plans.\n",
      "   - CrewAI's memory architecture is not explicitly mentioned in the provided context, so we can't compare it directly to LangChain's. However, since CrewAI focuses on role-based collaboration and ensuring each agent stays on task, it's unlikely that CrewAI agents use context-aware memory in the same way LangChain agents do.\n",
      "\n",
      "2. **Agent Architecture**:\n",
      "   - LangChain agents operate using a **planner-executor model**, which involves planning out a sequence of tool invocations to achieve a goal. This model allows for dynamic decision-making, branching logic, and context-aware memory use across steps.\n",
      "   - CrewAI agents are defined with a **purpose, a goal, and a set of tools they can use**. The framework ensures that each agent stays on task and contributes meaningfully to the overall crew objective. This implies a more rigid, goal-oriented architecture, where each agent is focused on a specific task or set of tasks.\n",
      "\n",
      "In summary, LangChain agents have a more dynamic, context-aware memory and architecture that allows for planning and decision-making across multiple steps. CrewAI agents, on the other hand, have a more straightforward, goal-oriented architecture that focuses on ensuring each agent contributes to the overall crew objective.\n",
      "\n",
      "Q: This sub-question shifts the focus towards the comparative aspect of the original question, allowing for document retrieval on the differences between LangChain and CrewAI\n",
      "A: Based on the context provided, here are the differences between LangChain and CrewAI in terms of document retrieval:\n",
      "\n",
      "1. **Retrieval Method**: LangChain supports hybrid retrieval, combining keyword-based (sparse) retrieval methods like BM25 with embedding-based (dense) retrieval. However, there is no mention of CrewAI's retrieval method in the provided context. It's possible that CrewAI might not have a built-in retrieval method, relying on LangChain for this purpose.\n",
      "\n",
      "2. **Tool Wrapping**: LangChain is capable of handling retrieval and tool wrapping, whereas CrewAI focuses on managing role-based collaboration. This suggests that CrewAI might not be directly involved in document retrieval, but rather complements LangChain's capabilities.\n",
      "\n",
      "3. **Hybrid Systems**: Both LangChain and CrewAI are designed to work together in hybrid systems, but their roles are different. LangChain handles retrieval and tool wrapping, while CrewAI manages role-based collaboration. This highlights the complementary nature of their functionality.\n",
      "\n",
      "4. **Collaboration vs. Retrieval**: The primary focus of CrewAI is on collaboration, whereas LangChain focuses on retrieval. CrewAI's role-based collaboration might be beneficial for tasks that require human input or decision-making, whereas LangChain's retrieval capabilities are suitable for tasks that require access to relevant information.\n",
      "\n",
      "In summary, while LangChain excels in document retrieval with its hybrid retrieval method and tool wrapping capabilities, CrewAI focuses on managing role-based collaboration, potentially relying on LangChain for retrieval tasks.\n",
      "\n",
      "Q: Optional sub-question:\n",
      "A: It seems like you forgot to ask a question. Please provide a question or a task that I can help you with based on the context you provided. I'll be happy to assist you once you do.\n",
      "\n",
      "Q: What are the key features and benefits of using LangChain's memory and agent architecture?\n",
      "A: Based on the provided context, the key features and benefits of using LangChain's memory and agent architecture are:\n",
      "\n",
      "1. **Dynamic Decision-Making**: LangChain's planner-executor model allows for dynamic decision-making, enabling agents to adjust their plans based on new information or changing circumstances.\n",
      "2. **Branching Logic**: Agents can use branching logic to handle multiple possible outcomes, allowing them to adapt to different scenarios.\n",
      "3. **Context-Aware Memory Use**: LangChain's memory architecture enables context-aware memory use across steps, allowing agents to retain and recall relevant information from previous steps.\n",
      "4. **Multi-Step Task Execution**: LangChain agents can execute complex multi-step tasks, integrating with various tools such as web search, calculators, and code execution.\n",
      "5. **LLM Integration**: Agents use Large Language Models (LLMs) to reason about which tool to call, what input to provide, and how to process the output, providing a powerful reasoning capability.\n",
      "\n",
      "These features and benefits enable LangChain's agents to perform complex tasks, adapt to changing situations, and make informed decisions with the help of LLMs.\n",
      "\n",
      "Q: This sub-question can help retrieve documents that compare the features and benefits of LangChain's memory and agent architecture, providing a more comprehensive understanding of the technology\n",
      "A: To answer the sub-question and retrieve documents that compare the features and benefits of LangChain's memory and agent architecture, you can try the following search:\n",
      "\n",
      "- Search for keywords: \"LangChain memory vs agent\", \"LangChain memory and agent architecture comparison\", \"LangChain memory and agent benefits\", or \"LangChain memory and agent features\".\n",
      "- Use academic databases and online libraries to find research papers and studies that compare LangChain's memory and agent architecture.\n",
      "- Visit LangChain's official documentation and GitHub repository to see if they have any documentation or examples that compare the memory and agent architecture.\n",
      "- Look for articles and blogs from industry experts and developers who have experience with LangChain to see if they have written about the benefits and features of the memory and agent architecture.\n",
      "\n",
      "Some possible search queries could be:\n",
      "\n",
      "- \"LangChain memory and agent architecture comparison\" site:github.com\n",
      "- \"LangChain memory vs agent\" filetype:pdf\n",
      "- \"LangChain memory and agent benefits\" site:langchain.readthedocs.io\n",
      "\n",
      "By searching for these keywords and visiting relevant websites, you should be able to find documents that compare the features and benefits of LangChain's memory and agent architecture.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Execute\n",
    "query=\"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "final_answer=query_decomposition_rag_pipeline(query)\n",
    "print(f\"âœ… Final answer:\\n\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dc978e",
   "metadata": {},
   "source": [
    "#### Major Disadvantage:\n",
    "Lots of LLM and retrieval calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672887f8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGUdemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
