{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce41d2b4",
   "metadata": {},
   "source": [
    "# Semantic Chunking\n",
    "\n",
    "- SemanticChunker document splitter that uses embedding similarity between sentences to decide chunk boundaries.\n",
    "- It ensures that each chunk is semantically coherent and not cut-off mid-thought like traditional character/token splitters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dd7ae6",
   "metadata": {},
   "source": [
    "### 1. Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e02ed42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neeladnatarajan/DSProjects/LLMOps/hw/RAGUdemy/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c288daa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Œ Semantic Chunks:\n",
      "\n",
      "Chunk1:\n",
      "LangChain is a framework for building applications with LLM. LangChain provides more modular abstractions to combine LLMs with tools like PineCone and OpenAI\n",
      "\n",
      "Chunk2:\n",
      "You can create chains, agents, memory and retriever\n",
      "\n",
      "Chunk3:\n",
      "The Eiffel Tower is located in Paris.\n",
      "\n",
      "Chunk4:\n",
      "France is a popular tourist destination.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model=SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "LangChain is a framework for building applications with LLM.\n",
    "LangChain provides more modular abstractions to combine LLMs with tools like PineCone and OpenAI\n",
    "You can create chains, agents, memory and retriever\n",
    "The Eiffel Tower is located in Paris.\n",
    "France is a popular tourist destination.\n",
    "\"\"\"\n",
    "\n",
    "# 1. Split into sentences\n",
    "sentences = [s.strip() for s in text.split(\"\\n\") if s.strip()]\n",
    "\n",
    "# 2. Sentence embeddings\n",
    "embeddings=model.encode(sentences)\n",
    "\n",
    "# 3. Initialize the parameters & hyperparameters\n",
    "threshold = 0.7\n",
    "chunks = []\n",
    "current_chunk=[sentences[0]]\n",
    "\n",
    "# 4. Semantic grouping based on threshold\n",
    "\n",
    "for i in range(1, len(sentences)):\n",
    "    sim = cosine_similarity(\n",
    "        [embeddings[i-1]],\n",
    "        [embeddings[i]]\n",
    "    )[0][0]\n",
    "\n",
    "    if sim >= threshold:\n",
    "        current_chunk.append(sentences[i])\n",
    "    else:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "        current_chunk=[sentences[i]]\n",
    "# Add the last chunk to the current chunk\n",
    "chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "# Output the chunks\n",
    "print(f\"\\nðŸ“Œ Semantic Chunks:\")\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    print(f\"\\nChunk{idx+1}:\\n{chunk}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67694af",
   "metadata": {},
   "source": [
    "### 2. RAG pipeline modularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3aa5bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableMap\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['GROQ_API_KEY'] = os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1508a10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom semantic chunker with threshold\n",
    "\n",
    "class ThresholdSemanticChunker:\n",
    "    def __init__(self, modelname=\"all-MiniLM-L6-v2\", threshold=0.7):\n",
    "        self.model=SentenceTransformer(modelname)\n",
    "        self.threshold=threshold\n",
    "    \n",
    "    def split_text(self,text:str):\n",
    "        \"\"\" Split the text into semantic chunks of sentences \"\"\"\n",
    "        \n",
    "        sentences=[s.strip() for s in text.split(\"\\n\") if s.strip()]\n",
    "        embeddings=self.model.encode(sentences)\n",
    "        chunks=[]\n",
    "        current_chunk=[sentences[0]]\n",
    "\n",
    "        for i in range(1, len(sentences)):\n",
    "            sim = cosine_similarity(\n",
    "                [embeddings[i-1]],\n",
    "                [embeddings[i]]\n",
    "                )[0][0]\n",
    "            if sim >= threshold:\n",
    "                current_chunk.append(sentences[i])\n",
    "            else:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk=[sentences[i]]\n",
    "            # Add the last chunk to the current chunk\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "        return chunks\n",
    "    \n",
    "    def split_docs(self, docs):\n",
    "        \"\"\" Split documents into semantic chunks of sentences \"\"\"\n",
    "        doc_chunks = []\n",
    "\n",
    "        for doc in docs:\n",
    "            print(doc.page_content)\n",
    "            print(doc.metadata)\n",
    "            for chunk in self.split_text(doc.page_content):\n",
    "                doc_chunks.append(Document(page_content=chunk, metadata=doc.metadata))\n",
    "        \n",
    "        return doc_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59fbf5e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'blog', 'page': 1}, page_content='\\nLangChain is a framework for building applications with LLM.\\nLangChain provides more modular abstractions to combine LLMs with tools like PineCone and OpenAI\\nYou can create chains, agents, memory and retriever\\nThe Eiffel Tower is located in Paris.\\nFrance is a popular tourist destination.\\n')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample text\n",
    "text = \"\"\"\n",
    "LangChain is a framework for building applications with LLM.\n",
    "LangChain provides more modular abstractions to combine LLMs with tools like PineCone and OpenAI\n",
    "You can create chains, agents, memory and retriever\n",
    "The Eiffel Tower is located in Paris.\n",
    "France is a popular tourist destination.\n",
    "\"\"\"\n",
    "doc = Document(\n",
    "    page_content=text,\n",
    "    metadata={\"source\":\"blog\", \"page\":1}\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a764b4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LangChain is a framework for building applications with LLM.\n",
      "LangChain provides more modular abstractions to combine LLMs with tools like PineCone and OpenAI\n",
      "You can create chains, agents, memory and retriever\n",
      "The Eiffel Tower is located in Paris.\n",
      "France is a popular tourist destination.\n",
      "\n",
      "{'source': 'blog', 'page': 1}\n",
      "[Document(metadata={'source': 'blog', 'page': 1}, page_content='LangChain is a framework for building applications with LLM. LangChain provides more modular abstractions to combine LLMs with tools like PineCone and OpenAI'), Document(metadata={'source': 'blog', 'page': 1}, page_content='You can create chains, agents, memory and retriever'), Document(metadata={'source': 'blog', 'page': 1}, page_content='The Eiffel Tower is located in Paris.'), Document(metadata={'source': 'blog', 'page': 1}, page_content='France is a popular tourist destination.')]\n"
     ]
    }
   ],
   "source": [
    "# Execute SemanticChunker\n",
    "chunker=ThresholdSemanticChunker(threshold=0.7)\n",
    "chunks = chunker.split_docs([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c680d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'blog', 'page': 1}, page_content='LangChain is a framework for building applications with LLM. LangChain provides more modular abstractions to combine LLMs with tools like PineCone and OpenAI'),\n",
       " Document(metadata={'source': 'blog', 'page': 1}, page_content='You can create chains, agents, memory and retriever'),\n",
       " Document(metadata={'source': 'blog', 'page': 1}, page_content='The Eiffel Tower is located in Paris.'),\n",
       " Document(metadata={'source': 'blog', 'page': 1}, page_content='France is a popular tourist destination.')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d7801fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorstore and retriever\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "embedding=OpenAIEmbeddings()\n",
    "vectorstore=FAISS.from_documents(chunks,embedding)\n",
    "retriever=vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9514951e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='You are a helpful AI assistant. Answer the question based on the provided context.\\n    If you cannot find the answer in the context, say \"I don\\'t have enough information to answer that question.\"\\n    Be concise and accurate in your responses. \\n\\n    Context: \\n    {context}\\n\\n    Question: {question}\\n    Answer:'), additional_kwargs={})])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create prompt template and prompt\n",
    "prompt_template=\"\"\"You are a helpful AI assistant. Answer the question based on the provided context.\n",
    "    If you cannot find the answer in the context, say \"I don't have enough information to answer that question.\"\n",
    "    Be concise and accurate in your responses. \n",
    "        \n",
    "    Context: \n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    Answer:\"\"\"\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(prompt_template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aa92caf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x3060eae50>, search_kwargs={})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0234af7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: RunnableLambda(...),\n",
       "  question: RunnableLambda(...)\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='You are a helpful AI assistant. Answer the question based on the provided context.\\n    If you cannot find the answer in the context, say \"I don\\'t have enough information to answer that question.\"\\n    Be concise and accurate in your responses. \\n\\n    Context: \\n    {context}\\n\\n    Question: {question}\\n    Answer:'), additional_kwargs={})])\n",
       "| ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 8192, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x133e7b390>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x133e7a090>, model_name='llama-3.1-8b-instant', temperature=0.4, model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create LLM and Build LCEL Chain\n",
    "llm=init_chat_model(model=\"groq:llama-3.1-8b-instant\", temperature=0.4)\n",
    "\n",
    "rag_chain = (\n",
    "    RunnableMap(\n",
    "        {\n",
    "        \"context\": lambda x: retriever.invoke(x['question']),\n",
    "        \"question\": lambda x: x['question'],\n",
    "        }\n",
    "    \n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7e02f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the RAG Chain with a question\n",
    "query = {\"question\": \"What is langchain used for?\"}\n",
    "result = rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2fcc15c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain is a framework for building applications with LLM (Large Language Model).'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a90d2be",
   "metadata": {},
   "source": [
    "### 3. Semantic Chunker with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "66567be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4822dd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk1: \n",
      "\n",
      "Content: LangChain is a framework for building applications with LLM. LangChain provides more modular abstractions to combine LLMs with tools like PineCone and OpenAI\n",
      "You can create chains, agents, memory and retriever\n",
      "The Eiffel Tower is located in Paris.\n",
      "\n",
      "Metadata: {'source': './langchain_intro.txt'}\n",
      "\n",
      "Chunk2: \n",
      "\n",
      "Content: France is a popular tourist destination.\n",
      "\n",
      "Metadata: {'source': './langchain_intro.txt'}\n"
     ]
    }
   ],
   "source": [
    "### Load the documents\n",
    "loader=TextLoader(\"./langchain_intro.txt\")\n",
    "docs=loader.load()\n",
    "\n",
    "#Initialize the embedding model\n",
    "embedding=OpenAIEmbeddings()\n",
    "\n",
    "# Create semantic chunker\n",
    "chunker=SemanticChunker(embedding)\n",
    "\n",
    "# Split the documents\n",
    "chunks=chunker.split_documents(docs)\n",
    "\n",
    "# Display chunks\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\nChunk{i+1}: \\n\")\n",
    "    print(f\"Content: {chunk.page_content}\")\n",
    "    print(f\"\\nMetadata: {chunk.metadata}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ef6da7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGUdemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
