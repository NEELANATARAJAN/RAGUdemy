{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38cdda75",
   "metadata": {},
   "source": [
    "# Hybrid Search Strategies\n",
    "#### Combining Dense and Sparse matrix for better context retrieval from vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c5dfd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All okay!\n"
     ]
    }
   ],
   "source": [
    "print(\"All okay!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90264d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_classic.retrievers import EnsembleRetriever\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56b78e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x141665e10>, search_kwargs={})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Sample documents\n",
    "docs = [\n",
    "    Document(page_content=\"LangChain helps build LLM applications.\"),\n",
    "    Document(page_content=\"Pinecone is a vector database for semantic search.\"),\n",
    "    Document(page_content=\"The Eiffel Tower is located in Paris\"),\n",
    "    Document(page_content=\"LangChain can be used to develop Agentic AI applications.\"),\n",
    "    Document(page_content=\"LangChain has many types of retrievers.\")\n",
    "]\n",
    "\n",
    "# Step 2: Dense retriever (FAISS + HuggingFace)\n",
    "embedding_model=HuggingFaceEmbeddings(model=\"all-MiniLM-L6-v2\")\n",
    "dense_vectorstore=FAISS.from_documents(docs,embedding_model)\n",
    "dense_retriever=dense_vectorstore.as_retriever()\n",
    "dense_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a872b8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer=<rank_bm25.BM25Okapi object at 0x1048925d0> k=3\n",
      "retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x141665e10>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x1048925d0>, k=3)] weights=[0.5, 0.5]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Sparse retriever (BM25)\n",
    "sparse_retriever=BM25Retriever.from_documents(docs)\n",
    "sparse_retriever.k=3\n",
    "\n",
    "print(sparse_retriever)\n",
    "\n",
    "# Step 4: Combine Dense and Sparse retriever into a Hybrid retriever\n",
    "hybrid_retriever=EnsembleRetriever(\n",
    "    retrievers=[dense_retriever,sparse_retriever],\n",
    "    weight=[0.7,0.3] ### alpha - hyperparamater\n",
    ")\n",
    "print(hybrid_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ee53920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Document 1: LangChain helps build LLM applications.\n",
      "\n",
      " Document 2: LangChain can be used to develop Agentic AI applications.\n",
      "\n",
      " Document 3: LangChain has many types of retrievers.\n",
      "\n",
      " Document 4: Pinecone is a vector database for semantic search.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Query and get the results\n",
    "query = \"How can I build an application using LLM?\"\n",
    "results = hybrid_retriever.invoke(query)\n",
    "\n",
    "# Step 6: Print results\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n Document {i+1}: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5812d148",
   "metadata": {},
   "source": [
    "#### RAG Pipeline with hybrid retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88d64535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_classic.chains.retrieval import create_retrieval_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "80342ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Prompt Template\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\" Answer the question based on the context below. \n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {input}\n",
    "\n",
    "    Answer:\"\"\"\n",
    ")\n",
    "# Step 8: LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-nano-2025-04-14\", temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e23f4521",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creat stuff document chain\n",
    "document_chain=create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "rag_chain=create_retrieval_chain(retriever=hybrid_retriever, combine_docs_chain=document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2f39ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: You can build apps using LLMs by leveraging frameworks like LangChain, which helps develop LLM applications and agentic AI systems. Additionally, integrating tools such as various retrievers for data retrieval and using vector databases like Pinecone for semantic search can enhance your application's capabilities.\n",
      "\n",
      "Source Documents:\n",
      "\n",
      "Doc 1: LangChain helps build LLM applications.\n",
      "\n",
      "Doc 2: LangChain can be used to develop Agentic AI applications.\n",
      "\n",
      "Doc 3: LangChain has many types of retrievers.\n",
      "\n",
      "Doc 4: Pinecone is a vector database for semantic search.\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Test the RAG Chain\n",
    "query={\"input\":\"How can I build apps using LLM?\"}\n",
    "response=rag_chain.invoke(query)\n",
    "\n",
    "# Step 10: Output\n",
    "print(f\"\\nAnswer: {response['answer']}\")\n",
    "print(\"\\nSource Documents:\")\n",
    "for i, doc in enumerate(response['context']):\n",
    "    print(f\"\\nDoc {i+1}: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492acc8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGUdemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
