{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "496ae752",
   "metadata": {},
   "source": [
    "# Re-Ranking Hybrid Search Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca50e5d",
   "metadata": {},
   "source": [
    "#### Re-ranking is second-stage filtering process in retrieval systems, especially in RAG pipelines, where we:\n",
    "- 1. First use a fast retriever(like BM25, FAISS, hybrid) to fetch top k records quickly.\n",
    "- 2. Then use a more accurate but slower model (like a cross-encoder or LLM) to re-score and reorder those documents by relevance to the query.\n",
    "#### ðŸ‘‰ It ensures most relevant document appears in the top thereby improving the LLM final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a79e9e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neeladnatarajan/DSProjects/LLMOps/hw/RAGUdemy/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_classic.chat_models import init_chat_model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27a4a069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'langchain_sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load text file\n",
    "loader=TextLoader(\"langchain_sample.txt\")\n",
    "raw_doc=loader.load()\n",
    "\n",
    "# Split documents into chunks\n",
    "splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs=splitter.split_documents(raw_doc)\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bda688cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## User query\n",
    "query=\"How can I use langchain to build an application with memory and tools?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71f4f27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FAISS vector store and HuggingFace models for embeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model=HuggingFaceEmbeddings(model=\"all-MiniLM-L6-v2\")\n",
    "vectorstore=FAISS.from_documents(docs, embedding_model)\n",
    "retriever=vectorstore.as_retriever(search_kwargs={\"k\":8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db658a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x169014b10>, search_kwargs={'k': 8})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90c59229",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reranking prompt and LLM\n",
    "from langchain_classic.chat_models import init_chat_model\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['GROQ_API_KEY']=os.getenv(\"GROQ_API_KEY\")\n",
    "llm=init_chat_model(\"groq:llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "802f70f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "prompt=PromptTemplate.from_template(\"\"\"You are a helpful assistant. Your task is to rank the follwing\n",
    "                                    from most to least relevant\n",
    "                                    \n",
    "                                    User Question: {question}\n",
    "                                    Document: {documents} \n",
    "                                    \n",
    "                                    Instructions:\n",
    "                                    - Think about the relevance of each document to the user's question.\n",
    "                                    - Return the list of document   indices in ranked order, starting from the most relevant.\n",
    "\n",
    "                                    Output format: comma separated document indices \n",
    "                                    \"\"\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "537baca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='3b42a25f-a2d2-42d4-82bb-b4cdb003e2e8', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.'),\n",
       " Document(id='4b103b8b-8237-47f4-ada8-87656a758f3b', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(id='8b709901-34d5-45d9-bf85-40d9a8933dbf', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.'),\n",
       " Document(id='d7d995ef-1f43-49b9-82e7-66c49746a5a9', metadata={'source': 'langchain_sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(id='b6e90e31-7ff4-4e0e-8cbc-5ed68a47356c', metadata={'source': 'langchain_sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.'),\n",
       " Document(id='e5b93dc7-7dc2-45e4-9ee5-217d4c2fd6a1', metadata={'source': 'langchain_sample.txt'}, page_content='Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs=retriever.invoke(query)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f92d177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lines=[f\"{i+1}, {doc.page_content}\" for i, doc in enumerate(retrieved_docs)]\n",
    "formatted_docs=\"\\n\".join(doc_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0f97c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the user's question, I would rank the documents from most to least relevant as follows:\n",
      "\n",
      "3, 1, 2, 5, 4, 6\n",
      "\n",
      "Here's the reasoning behind the ranking:\n",
      "\n",
      "- Document 3 directly mentions tool integration, which is the key aspect of the user's question.\n",
      "- Document 1 explains memory in LangChain, which is a crucial component for building a stateful application, aligning with the user's goal.\n",
      "- Document 2 provides a general overview of LangChain, but it's still relevant as it mentions components that are relevant to the user's question.\n",
      "- Document 5 discusses Retrieval-Augmented Generation (RAG), which is a specific technique that can be used in building an application with memory and tools.\n",
      "- Document 4 is about FAISS, a library that's mentioned in Document 5, but it's not directly related to the user's question.\n",
      "- Document 6 discusses dense retrieval, which is a specific retrieval method that's mentioned in Document 5, but it's not as directly relevant to the user's question as the other documents.\n",
      "\n",
      "Note that the ranking is not necessarily an exact measure of relevance, but rather a subjective interpretation based on the content of the documents.\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|llm|StrOutputParser()\n",
    "response=chain.invoke({\"question\":query, \"documents\": formatted_docs })\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c41ca6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 3]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 7: Parse and rerank\n",
    "indices=[int(x.strip())-1 for x in response.split(\",\") if x.strip().isdigit()]\n",
    "indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6f3c082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='3b42a25f-a2d2-42d4-82bb-b4cdb003e2e8', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.'),\n",
       " Document(id='4b103b8b-8237-47f4-ada8-87656a758f3b', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(id='b6e90e31-7ff4-4e0e-8cbc-5ed68a47356c', metadata={'source': 'langchain_sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.'),\n",
       " Document(id='d7d995ef-1f43-49b9-82e7-66c49746a5a9', metadata={'source': 'langchain_sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranked_docs=[retrieved_docs[i] for i in indices if 0<=i<len(retrieved_docs)]\n",
    "reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1174b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Final Reranked result:\n",
      "\n",
      "\n",
      "Rank 1: \n",
      "LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\n",
      "Memory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.\n",
      "\n",
      "Rank 2: \n",
      "LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.\n",
      "\n",
      "Rank 3: \n",
      "Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\n",
      "BM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.\n",
      "\n",
      "Rank 4: \n",
      "FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\n",
      "Agents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.\n"
     ]
    }
   ],
   "source": [
    "# Show result\n",
    "print(\"\\n Final Reranked result:\\n\")\n",
    "for i, doc in enumerate(reranked_docs, 1):\n",
    "    print(f\"\\nRank {i}: \\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b160db4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGUdemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
